{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A generic explainability architecture for explaining text machine learning models. Marcel Robeer, 2021 Installation See installation.md for an extended installation guide. Method Instructions pip Install from PyPI via pip3 install text_explainability . Local Clone this repository and install via pip3 install -e . or locally run python3 setup.py install . Example usage See example_usage.md to see an example of how the package can be used, or run the lines in example_usage.py to do explore it interactively. Explanation methods included text_explainability includes methods for model-agnostic local explanation and global explanation . Each of these methods can be fully customized to fit the explainees' needs. Type Explanation method Description Paper/link Local explanation LIME Calculate feature attribution with Local Intepretable Model-Agnostic Explanations (LIME). [ Ribeiro2016 ], interpretable-ml/lime KernelSHAP Calculate feature attribution with Shapley Additive Explanations (SHAP). [ Lundberg2017 ], interpretable-ml/shap LocalTree Fit a local decision tree around a single decision. [ Guidotti2018 ] LocalRules Fit a local sparse set of label-specific rules using SkopeRules . github/skope-rules FoilTree Fit a local contrastive/counterfactual decision tree around a single decision. [ Robeer2018 ] Global explanation TokenFrequency Show the top- k number of tokens for each ground-truth or predicted label. TokenInformation Show the top- k token mutual information for a dataset or model. wikipedia/mutual_information KMedoids Embed instances and find top- n prototypes (can also be performed for each label using LabelwiseKMedoids ). interpretable-ml/prototypes MMDCritic Embed instances and find top- n prototypes and top- n criticisms (can also be performed for each label using LabelwiseMMDCritic ). [ Kim2016 ], interpretable-ml/prototypes Releases text_explainability is officially released through PyPI . See CHANGELOG.md for a full overview of the changes for each version. Maintenance Contributors Marcel Robeer ( @m.j.robeer ) Michiel Bron ( @mpbron-phd ) Todo Tasks yet to be done: Implement local post-hoc explanations: Implement Anchors Implement global post-hoc explanations: Representative subset Add support for regression models More complex data augmentation Top-k replacement (e.g. according to LM / WordNet) Tokens to exclude from being changed Bag-of-words style replacements Add rule-based return type Write more tests","title":"Home"},{"location":"#installation","text":"See installation.md for an extended installation guide. Method Instructions pip Install from PyPI via pip3 install text_explainability . Local Clone this repository and install via pip3 install -e . or locally run python3 setup.py install .","title":"Installation"},{"location":"#example-usage","text":"See example_usage.md to see an example of how the package can be used, or run the lines in example_usage.py to do explore it interactively.","title":"Example usage"},{"location":"#explanation-methods-included","text":"text_explainability includes methods for model-agnostic local explanation and global explanation . Each of these methods can be fully customized to fit the explainees' needs. Type Explanation method Description Paper/link Local explanation LIME Calculate feature attribution with Local Intepretable Model-Agnostic Explanations (LIME). [ Ribeiro2016 ], interpretable-ml/lime KernelSHAP Calculate feature attribution with Shapley Additive Explanations (SHAP). [ Lundberg2017 ], interpretable-ml/shap LocalTree Fit a local decision tree around a single decision. [ Guidotti2018 ] LocalRules Fit a local sparse set of label-specific rules using SkopeRules . github/skope-rules FoilTree Fit a local contrastive/counterfactual decision tree around a single decision. [ Robeer2018 ] Global explanation TokenFrequency Show the top- k number of tokens for each ground-truth or predicted label. TokenInformation Show the top- k token mutual information for a dataset or model. wikipedia/mutual_information KMedoids Embed instances and find top- n prototypes (can also be performed for each label using LabelwiseKMedoids ). interpretable-ml/prototypes MMDCritic Embed instances and find top- n prototypes and top- n criticisms (can also be performed for each label using LabelwiseMMDCritic ). [ Kim2016 ], interpretable-ml/prototypes","title":"Explanation methods included"},{"location":"#releases","text":"text_explainability is officially released through PyPI . See CHANGELOG.md for a full overview of the changes for each version.","title":"Releases"},{"location":"#maintenance","text":"","title":"Maintenance"},{"location":"#contributors","text":"Marcel Robeer ( @m.j.robeer ) Michiel Bron ( @mpbron-phd )","title":"Contributors"},{"location":"#todo","text":"Tasks yet to be done: Implement local post-hoc explanations: Implement Anchors Implement global post-hoc explanations: Representative subset Add support for regression models More complex data augmentation Top-k replacement (e.g. according to LM / WordNet) Tokens to exclude from being changed Bag-of-words style replacements Add rule-based return type Write more tests","title":"Todo"},{"location":"CHANGELOG/","text":"Changelog All notable changes to text_explainability will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased 0.4.5 - 2021-09-24 Added Decorator to allow strings to be converted into TextInstances Decorator to ensure TextInstances are tokenized when required Changed Typing fixes 0.4.4 - 2021-09-23 Added Character-level tokenizer/detokenizer 0.4.3 - 2021-09-20 Added New embeddings not requiring internet ( CountVectorizer , TfidfVectorizer ) Rules return type First version of local rules using SkopeRules More test cases Changed New default embedding method for MMDCritic and KMedoids Version moved to __init__.py New README.md layout Updates to Anchor local explanations Added random state in example_usage to ensure reproducibility 0.4.2 - 2021-09-13 Changed Hotfix to fix predict_proba usage 0.4.1 - 2021-09-13 Changed Hotfix to make dependency on internet optional 0.4.0 - 2021-09-13 Added Initial support for embeddings/vectors Support for dimensionality reduction Initial implementation of MMD-Critic Initial implementation of labelwise MMD-Critic Initial implementation of prototype selection using k-Medoids Changed Updated README.md 0.3.8 - 2021-09-07 Changed Bugfix in including locale/*.json files during setup Support for dimensionality reduction 0.3.7 - 2021-09-07 Added Dependencies for package 0.3.6 - 2021-09-07 Added PyPI release script to .gitignore Badges to README.md Added dependencies to setup.py 0.3.5 - 2021-09-03 Changed Bugfix for getting key in TokenFrequency Locale changed to .json format, to remove optional dependency Bugfixes in FeatureAttribution return type Bugfixes in i18n 0.3.4 - 2021-08-18 Changed External logo url Hotfix in FeatureAttribution 0.3.3 - 2021-08-18 Added Updated to support instancelib==0.3.1.2 i18n internationalization support CHANGELOG.md Changed Additional samples in example dataset Bugfixes for LIME and FeatureAttribution return type 0.3.2 - 2021-07-27 Added Initial support for Foil Trees Logo in documentation Changed Improved documentation 0.3.1 - 2021-07-23 Added flake8 linting CI/CD Pipeline Run test scripts 0.3.0 - 2021-07-20 Added Updated to support instancelib==0.3.0.0 Changed Improved documentation global_explanation classes have equal return types 0.2 - 2021-06-22 Added LICENSE.md Updated to support instancelib==0.2.3.1 Changed Module description 0.1 - 2021-05-28 Added README.md Example usage Local explanation classes (LIME, KernelSHAP) Global explanation classes Data augmentation/sampling Feature selection Local surrogates Tokenization git setup","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All notable changes to text_explainability will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"CHANGELOG/#unreleased","text":"","title":"Unreleased"},{"location":"CHANGELOG/#045-2021-09-24","text":"","title":"0.4.5 - 2021-09-24"},{"location":"CHANGELOG/#added","text":"Decorator to allow strings to be converted into TextInstances Decorator to ensure TextInstances are tokenized when required","title":"Added"},{"location":"CHANGELOG/#changed","text":"Typing fixes","title":"Changed"},{"location":"CHANGELOG/#044-2021-09-23","text":"","title":"0.4.4 - 2021-09-23"},{"location":"CHANGELOG/#added_1","text":"Character-level tokenizer/detokenizer","title":"Added"},{"location":"CHANGELOG/#043-2021-09-20","text":"","title":"0.4.3 - 2021-09-20"},{"location":"CHANGELOG/#added_2","text":"New embeddings not requiring internet ( CountVectorizer , TfidfVectorizer ) Rules return type First version of local rules using SkopeRules More test cases","title":"Added"},{"location":"CHANGELOG/#changed_1","text":"New default embedding method for MMDCritic and KMedoids Version moved to __init__.py New README.md layout Updates to Anchor local explanations Added random state in example_usage to ensure reproducibility","title":"Changed"},{"location":"CHANGELOG/#042-2021-09-13","text":"","title":"0.4.2 - 2021-09-13"},{"location":"CHANGELOG/#changed_2","text":"Hotfix to fix predict_proba usage","title":"Changed"},{"location":"CHANGELOG/#041-2021-09-13","text":"","title":"0.4.1 - 2021-09-13"},{"location":"CHANGELOG/#changed_3","text":"Hotfix to make dependency on internet optional","title":"Changed"},{"location":"CHANGELOG/#040-2021-09-13","text":"","title":"0.4.0 - 2021-09-13"},{"location":"CHANGELOG/#added_3","text":"Initial support for embeddings/vectors Support for dimensionality reduction Initial implementation of MMD-Critic Initial implementation of labelwise MMD-Critic Initial implementation of prototype selection using k-Medoids","title":"Added"},{"location":"CHANGELOG/#changed_4","text":"Updated README.md","title":"Changed"},{"location":"CHANGELOG/#038-2021-09-07","text":"","title":"0.3.8 - 2021-09-07"},{"location":"CHANGELOG/#changed_5","text":"Bugfix in including locale/*.json files during setup Support for dimensionality reduction","title":"Changed"},{"location":"CHANGELOG/#037-2021-09-07","text":"","title":"0.3.7 - 2021-09-07"},{"location":"CHANGELOG/#added_4","text":"Dependencies for package","title":"Added"},{"location":"CHANGELOG/#036-2021-09-07","text":"","title":"0.3.6 - 2021-09-07"},{"location":"CHANGELOG/#added_5","text":"PyPI release script to .gitignore Badges to README.md Added dependencies to setup.py","title":"Added"},{"location":"CHANGELOG/#035-2021-09-03","text":"","title":"0.3.5 - 2021-09-03"},{"location":"CHANGELOG/#changed_6","text":"Bugfix for getting key in TokenFrequency Locale changed to .json format, to remove optional dependency Bugfixes in FeatureAttribution return type Bugfixes in i18n","title":"Changed"},{"location":"CHANGELOG/#034-2021-08-18","text":"","title":"0.3.4 - 2021-08-18"},{"location":"CHANGELOG/#changed_7","text":"External logo url Hotfix in FeatureAttribution","title":"Changed"},{"location":"CHANGELOG/#033-2021-08-18","text":"","title":"0.3.3 - 2021-08-18"},{"location":"CHANGELOG/#added_6","text":"Updated to support instancelib==0.3.1.2 i18n internationalization support CHANGELOG.md","title":"Added"},{"location":"CHANGELOG/#changed_8","text":"Additional samples in example dataset Bugfixes for LIME and FeatureAttribution return type","title":"Changed"},{"location":"CHANGELOG/#032-2021-07-27","text":"","title":"0.3.2 - 2021-07-27"},{"location":"CHANGELOG/#added_7","text":"Initial support for Foil Trees Logo in documentation","title":"Added"},{"location":"CHANGELOG/#changed_9","text":"Improved documentation","title":"Changed"},{"location":"CHANGELOG/#031-2021-07-23","text":"","title":"0.3.1 - 2021-07-23"},{"location":"CHANGELOG/#added_8","text":"flake8 linting CI/CD Pipeline Run test scripts","title":"Added"},{"location":"CHANGELOG/#030-2021-07-20","text":"","title":"0.3.0 - 2021-07-20"},{"location":"CHANGELOG/#added_9","text":"Updated to support instancelib==0.3.0.0","title":"Added"},{"location":"CHANGELOG/#changed_10","text":"Improved documentation global_explanation classes have equal return types","title":"Changed"},{"location":"CHANGELOG/#02-2021-06-22","text":"","title":"0.2 - 2021-06-22"},{"location":"CHANGELOG/#added_10","text":"LICENSE.md Updated to support instancelib==0.2.3.1","title":"Added"},{"location":"CHANGELOG/#changed_11","text":"Module description","title":"Changed"},{"location":"CHANGELOG/#01-2021-05-28","text":"","title":"0.1 - 2021-05-28"},{"location":"CHANGELOG/#added_11","text":"README.md Example usage Local explanation classes (LIME, KernelSHAP) Global explanation classes Data augmentation/sampling Feature selection Local surrogates Tokenization git setup","title":"Added"},{"location":"example_usage/","text":"Example Usage Dependencies text_explainability uses instances and machine learning models wrapped with the InstanceLib library. 1 2 3 4 import os from instancelib.ingest.spreadsheet import read_csv_dataset from instancelib.instances.text import MemoryTextInstance Dataset and model As a dummy black-box model, we use the example dataset in ./datasets/test.csv and train a machine learning model on it with scikit-learn . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer from sklearn.ensemble import RandomForestClassifier from instancelib.machinelearning import SkLearnDataClassifier # Create train/test dataset path = os . path . join ( os . path . dirname ( __file__ ), './datasets/test.csv' ) test_env = read_csv_dataset ( path , data_cols = [ 'fulltext' ], label_cols = [ 'label' ]) instanceprovider = test_env . dataset labelprovider = test_env . labels train , test = test_env . train_test_split ( instanceprovider , train_size = 0.70 ) # Create sklearn model with pipeline p = Pipeline ([( 'vect' , CountVectorizer ()), ( 'tfidf' , TfidfTransformer ( use_idf = False )), ( 'rf' , RandomForestClassifier ( random_state = 0 )) ]) # Build and fit (train) model model = SkLearnDataClassifier . build ( p , test_env ) model . fit_provider ( train , labelprovider ) Using Text Explainability Text Explainability is used for local explanations (explaining a single prediction) or global explanations (explaining general dataset/model behavior). Local explanations Popular local explanations include LIME , KernelSHAP , local decion trees ( LocalTree ), local decision rules ( LocalRules ) and FoilTree . First, let us create a sample to explain: 1 2 3 4 from text_explainability import default_tokenizer data = 'Dit is zeer positieve proef...' sample = MemoryTextInstance ( 0 , data , None , tokenized = default_tokenizer ( data )) Next, the prediction of model on sample can be explained by generating neighborhood data ( text_explainability.data.augmentation.TokenReplacement ), used by LIME , LocalTree , FoilTree and KernelSHAP : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from text_explainability import LIME , LocalTree , FoilTree , KernelSHAP # LIME explainer for `sample` on `model` explainer = LIME ( test_env ) explainer ( sample , model , labels = [ 'neutraal' , 'positief' ]) . scores # SHAP explanation for `sample` on `model`, limited to 4 features KernelSHAP ( label_names = labelprovider )( sample , model , n_samples = 50 , l1_reg = 4 ) # Local tree explainer for `sample` on `model` (non-weighted neighborhood data) LocalTree ()( sample , model , weigh_samples = False ) # Contrastive local tree explainer for `sample` on `model` (why not 'positief'?) FoilTree ()( sample , model , foil_fn = 'positief' ) . rules # %% LocalRules on `model` (why 'positief'?) LocalRules ()( sample , model , foil_fn = 'negatief' , n_samples = 100 ) . rules Global explanations Global explanations provide information on the dataset and its ground-truth labels, or the dataset and corresponding predictions by the model . Example global explanations are TokenFrequency (the frequency of each token per label/class/bucket) or TokenInformation (how informative each token is for predicting the various labels). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from text_explainability import TokenFrequency , TokenInformation # Global word frequency explanation on ground-truth labels tf = TokenFrequency ( instanceprovider ) tf ( labelprovider = labelprovider , explain_model = False , k = 10 ) # Global word frequency explanation on model predictions tf ( model = model , explain_model = True , k = 3 , filter_words = PUNCTUATION ) # Token information for dataset ti = TokenInformation ( instanceprovider ) ti ( labelprovider = labelprovider , explain_model = False , k = 50 ) . scores # Token information for model ti ( model = model , explain_model = True , k = 50 , filter_words = PUNCTUATION ) Global explanation: Explanation by example Explanations by example provide information on a dataset (e.g. the test set) or subsets thereof (e.g. all training instances with label 0) by showing representative instances. Examples of representative instances are prototypes ( n most representative instances, e.g. of a class) and criticsms ( n instances not well represented by prototypes). Example explanations by example are KMedoids (using the k-Medoids algorithm to extract prototypes) and MMDCritic (extracting prototypes and corresponding criticisms). In addition, each of these can be performed labelwise (e.g. for the ground-truth labels in a labelprovider or for each models' predicted class). 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability import KMedoids , MMDCritic , LabelwiseMMDCritic # Extract top-2 prototypes with KMedoids KMedoids ( instanceprovider ) . prototypes ( n = 2 ) # Extract top-2 prototypes and top-2 criticisms label with MMDCritic MMDCritic ( instanceprovider )( n_prototypes = 2 , n_criticisms = 2 ) # Extract 1 prototype for each ground-truth label with MMDCritic LabelwiseMMDCritic ( instanceprovider , labelprovider ) . prototypes ( n = 1 ) # %% Extract 1 prototype and 2 criticisms for each predicted label with MMDCritic LabelwiseMMDCritic ( instanceprovider , model )( n_prototypes = 1 , n_criticisms = 2 )","title":"Example usage"},{"location":"example_usage/#example-usage","text":"","title":"Example Usage"},{"location":"example_usage/#dependencies","text":"text_explainability uses instances and machine learning models wrapped with the InstanceLib library. 1 2 3 4 import os from instancelib.ingest.spreadsheet import read_csv_dataset from instancelib.instances.text import MemoryTextInstance","title":"Dependencies"},{"location":"example_usage/#dataset-and-model","text":"As a dummy black-box model, we use the example dataset in ./datasets/test.csv and train a machine learning model on it with scikit-learn . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer from sklearn.ensemble import RandomForestClassifier from instancelib.machinelearning import SkLearnDataClassifier # Create train/test dataset path = os . path . join ( os . path . dirname ( __file__ ), './datasets/test.csv' ) test_env = read_csv_dataset ( path , data_cols = [ 'fulltext' ], label_cols = [ 'label' ]) instanceprovider = test_env . dataset labelprovider = test_env . labels train , test = test_env . train_test_split ( instanceprovider , train_size = 0.70 ) # Create sklearn model with pipeline p = Pipeline ([( 'vect' , CountVectorizer ()), ( 'tfidf' , TfidfTransformer ( use_idf = False )), ( 'rf' , RandomForestClassifier ( random_state = 0 )) ]) # Build and fit (train) model model = SkLearnDataClassifier . build ( p , test_env ) model . fit_provider ( train , labelprovider )","title":"Dataset and model"},{"location":"example_usage/#using-text-explainability","text":"Text Explainability is used for local explanations (explaining a single prediction) or global explanations (explaining general dataset/model behavior).","title":"Using Text Explainability"},{"location":"example_usage/#local-explanations","text":"Popular local explanations include LIME , KernelSHAP , local decion trees ( LocalTree ), local decision rules ( LocalRules ) and FoilTree . First, let us create a sample to explain: 1 2 3 4 from text_explainability import default_tokenizer data = 'Dit is zeer positieve proef...' sample = MemoryTextInstance ( 0 , data , None , tokenized = default_tokenizer ( data )) Next, the prediction of model on sample can be explained by generating neighborhood data ( text_explainability.data.augmentation.TokenReplacement ), used by LIME , LocalTree , FoilTree and KernelSHAP : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from text_explainability import LIME , LocalTree , FoilTree , KernelSHAP # LIME explainer for `sample` on `model` explainer = LIME ( test_env ) explainer ( sample , model , labels = [ 'neutraal' , 'positief' ]) . scores # SHAP explanation for `sample` on `model`, limited to 4 features KernelSHAP ( label_names = labelprovider )( sample , model , n_samples = 50 , l1_reg = 4 ) # Local tree explainer for `sample` on `model` (non-weighted neighborhood data) LocalTree ()( sample , model , weigh_samples = False ) # Contrastive local tree explainer for `sample` on `model` (why not 'positief'?) FoilTree ()( sample , model , foil_fn = 'positief' ) . rules # %% LocalRules on `model` (why 'positief'?) LocalRules ()( sample , model , foil_fn = 'negatief' , n_samples = 100 ) . rules","title":"Local explanations"},{"location":"example_usage/#global-explanations","text":"Global explanations provide information on the dataset and its ground-truth labels, or the dataset and corresponding predictions by the model . Example global explanations are TokenFrequency (the frequency of each token per label/class/bucket) or TokenInformation (how informative each token is for predicting the various labels). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from text_explainability import TokenFrequency , TokenInformation # Global word frequency explanation on ground-truth labels tf = TokenFrequency ( instanceprovider ) tf ( labelprovider = labelprovider , explain_model = False , k = 10 ) # Global word frequency explanation on model predictions tf ( model = model , explain_model = True , k = 3 , filter_words = PUNCTUATION ) # Token information for dataset ti = TokenInformation ( instanceprovider ) ti ( labelprovider = labelprovider , explain_model = False , k = 50 ) . scores # Token information for model ti ( model = model , explain_model = True , k = 50 , filter_words = PUNCTUATION )","title":"Global explanations"},{"location":"example_usage/#global-explanation-explanation-by-example","text":"Explanations by example provide information on a dataset (e.g. the test set) or subsets thereof (e.g. all training instances with label 0) by showing representative instances. Examples of representative instances are prototypes ( n most representative instances, e.g. of a class) and criticsms ( n instances not well represented by prototypes). Example explanations by example are KMedoids (using the k-Medoids algorithm to extract prototypes) and MMDCritic (extracting prototypes and corresponding criticisms). In addition, each of these can be performed labelwise (e.g. for the ground-truth labels in a labelprovider or for each models' predicted class). 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability import KMedoids , MMDCritic , LabelwiseMMDCritic # Extract top-2 prototypes with KMedoids KMedoids ( instanceprovider ) . prototypes ( n = 2 ) # Extract top-2 prototypes and top-2 criticisms label with MMDCritic MMDCritic ( instanceprovider )( n_prototypes = 2 , n_criticisms = 2 ) # Extract 1 prototype for each ground-truth label with MMDCritic LabelwiseMMDCritic ( instanceprovider , labelprovider ) . prototypes ( n = 1 ) # %% Extract 1 prototype and 2 criticisms for each predicted label with MMDCritic LabelwiseMMDCritic ( instanceprovider , model )( n_prototypes = 1 , n_criticisms = 2 )","title":"Global explanation: Explanation by example"},{"location":"docs/installation/","text":"Installation Installation of text_explainability requires Python 3.8 or higher. 1. Python installation Install Python on your operating system using the Python Setup and Usage guide. 2. Installing text_explainability text_explainability can be installed: - using pip : pip3 install (released on PyPI ) - locally : cloning the repository and using python3 setup.py install Using pip Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe Run the command: pip3 install text_explainability , or pip install text_explainability . 1 2 3 4 5 user@terminal:~$ pip3 install text_explainability Collecting text_explainability ... Installing collected packages: text-explainability Successfully installed text-explainability Locally Download the folder from GitLab/GitHub : Clone this repository, or Download it as a .zip file and extract it. Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe and navigate to the folder you downloaded text_explainability in. In the main folder (containing the setup.py file) run: python3 setup.py install , or python setup.py install . 1 2 3 4 5 6 7 user@terminal:~$ cd ~/text_explainability user@terminal:~/text_explanability$ python3 setup.py install running install running bdist_egg running egg_info ... Finished processing dependencies for text-explainability","title":"Installation"},{"location":"docs/installation/#installation","text":"Installation of text_explainability requires Python 3.8 or higher.","title":"Installation"},{"location":"docs/installation/#1-python-installation","text":"Install Python on your operating system using the Python Setup and Usage guide.","title":"1. Python installation"},{"location":"docs/installation/#2-installing-text_explainability","text":"text_explainability can be installed: - using pip : pip3 install (released on PyPI ) - locally : cloning the repository and using python3 setup.py install","title":"2. Installing text_explainability"},{"location":"docs/installation/#using-pip","text":"Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe Run the command: pip3 install text_explainability , or pip install text_explainability . 1 2 3 4 5 user@terminal:~$ pip3 install text_explainability Collecting text_explainability ... Installing collected packages: text-explainability Successfully installed text-explainability","title":"Using pip"},{"location":"docs/installation/#locally","text":"Download the folder from GitLab/GitHub : Clone this repository, or Download it as a .zip file and extract it. Open up a terminal (Linux / macOS) or cmd.exe / powershell.exe and navigate to the folder you downloaded text_explainability in. In the main folder (containing the setup.py file) run: python3 setup.py install , or python setup.py install . 1 2 3 4 5 6 7 user@terminal:~$ cd ~/text_explainability user@terminal:~/text_explanability$ python3 setup.py install running install running bdist_egg running egg_info ... Finished processing dependencies for text-explainability","title":"Locally"},{"location":"reference/text_explainability/","text":"Module text_explainability None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability.global_explanation import (TokenFrequency, TokenInformation, KMedoids, LabelwiseKMedoids, MMDCritic, LabelwiseMMDCritic) from text_explainability.local_explanation import LIME, KernelSHAP, Anchor, LocalTree from text_explainability.utils import (default_tokenizer, default_detokenizer, word_tokenizer, word_detokenizer, character_tokenizer, character_detokenizer) __version__ = '0.4.5' Sub-modules text_explainability.data text_explainability.decorators text_explainability.default text_explainability.generation text_explainability.global_explanation text_explainability.internationalization text_explainability.local_explanation text_explainability.locale text_explainability.utils","title":"Index"},{"location":"reference/text_explainability/#module-text_explainability","text":"None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 from text_explainability.global_explanation import (TokenFrequency, TokenInformation, KMedoids, LabelwiseKMedoids, MMDCritic, LabelwiseMMDCritic) from text_explainability.local_explanation import LIME, KernelSHAP, Anchor, LocalTree from text_explainability.utils import (default_tokenizer, default_detokenizer, word_tokenizer, word_detokenizer, character_tokenizer, character_detokenizer) __version__ = '0.4.5'","title":"Module text_explainability"},{"location":"reference/text_explainability/#sub-modules","text":"text_explainability.data text_explainability.decorators text_explainability.default text_explainability.generation text_explainability.global_explanation text_explainability.internationalization text_explainability.local_explanation text_explainability.locale text_explainability.utils","title":"Sub-modules"},{"location":"reference/text_explainability/decorators/","text":"Module text_explainability.decorators Function decorators to ensure functions are fool-proof en readable. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \"\"\"Function decorators to ensure functions are fool-proof en readable.\"\"\" import inspect from functools import wraps, partial from uuid import uuid4 from instancelib.instances.text import MemoryTextInstance from text_explainability.utils import default_tokenizer def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = MemoryTextInstance(str(uuid4()), arg, None) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner Functions text_instance 1 2 3 4 5 def text_instance ( func = None , * , tokenize : bool = False ) Decorator to convert an accidentally passed string to a TextInstance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = MemoryTextInstance(str(uuid4()), arg, None) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"Decorators"},{"location":"reference/text_explainability/decorators/#module-text_explainabilitydecorators","text":"Function decorators to ensure functions are fool-proof en readable. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \"\"\"Function decorators to ensure functions are fool-proof en readable.\"\"\" import inspect from functools import wraps, partial from uuid import uuid4 from instancelib.instances.text import MemoryTextInstance from text_explainability.utils import default_tokenizer def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = MemoryTextInstance(str(uuid4()), arg, None) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"Module text_explainability.decorators"},{"location":"reference/text_explainability/decorators/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/decorators/#text_instance","text":"1 2 3 4 5 def text_instance ( func = None , * , tokenize : bool = False ) Decorator to convert an accidentally passed string to a TextInstance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def text_instance(func=None, *, tokenize: bool = False): \"\"\"Decorator to convert an accidentally passed string to a TextInstance.\"\"\" if func is None: return partial(text_instance, tokenize=tokenize) def str_to_text_instance(arg): if isinstance(arg, str): arg = MemoryTextInstance(str(uuid4()), arg, None) if tokenize and not arg.tokenized: arg.tokenized = default_tokenizer(arg.data) return arg @wraps(func) def inner(*args, **kwargs): possible_args = [i for i, t in enumerate(inspect.signature(func).parameters.values()) if 'TextInstance' in str(t)] if possible_args: args = tuple(str_to_text_instance(a) if j in possible_args else a for j, a in enumerate(list(args))) return func(*args, **kwargs) return inner","title":"text_instance"},{"location":"reference/text_explainability/default/","text":"Module text_explainability.default Default classes for all to inherit from. None View Source 1 2 3 4 5 6 7 8 9 10 11 \"\"\"Default classes for all to inherit from.\"\"\" class Readable: \"\"\"Ensure that a class has a readable representation.\"\"\" def __repr__(self): public_vars = ', '.join([f'{k}={v}' for k, v in vars(self).items() if not k.startswith('_')]) return f'{self.__class__.__name__}({public_vars})' Classes Readable 1 2 3 4 5 class Readable ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 class Readable: \"\"\"Ensure that a class has a readable representation.\"\"\" def __repr__(self): public_vars = ', '.join([f'{k}={v}' for k, v in vars(self).items() if not k.startswith('_')]) return f'{self.__class__.__name__}({public_vars})' Descendants text_explainability.generation.surrogate.BaseSurrogate text_explainability.data.embedding.Embedder text_explainability.data.sampling.PrototypeSampler text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.global_explanation.GlobalExplanation text_explainability.data.augmentation.LocalTokenPertubator text_explainability.generation.feature_selection.FeatureSelector text_explainability.local_explanation.LocalExplanation","title":"Default"},{"location":"reference/text_explainability/default/#module-text_explainabilitydefault","text":"Default classes for all to inherit from. None View Source 1 2 3 4 5 6 7 8 9 10 11 \"\"\"Default classes for all to inherit from.\"\"\" class Readable: \"\"\"Ensure that a class has a readable representation.\"\"\" def __repr__(self): public_vars = ', '.join([f'{k}={v}' for k, v in vars(self).items() if not k.startswith('_')]) return f'{self.__class__.__name__}({public_vars})'","title":"Module text_explainability.default"},{"location":"reference/text_explainability/default/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/default/#readable","text":"1 2 3 4 5 class Readable ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 class Readable: \"\"\"Ensure that a class has a readable representation.\"\"\" def __repr__(self): public_vars = ', '.join([f'{k}={v}' for k, v in vars(self).items() if not k.startswith('_')]) return f'{self.__class__.__name__}({public_vars})'","title":"Readable"},{"location":"reference/text_explainability/default/#descendants","text":"text_explainability.generation.surrogate.BaseSurrogate text_explainability.data.embedding.Embedder text_explainability.data.sampling.PrototypeSampler text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.global_explanation.GlobalExplanation text_explainability.data.augmentation.LocalTokenPertubator text_explainability.generation.feature_selection.FeatureSelector text_explainability.local_explanation.LocalExplanation","title":"Descendants"},{"location":"reference/text_explainability/global_explanation/","text":"Module text_explainability.global_explanation Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 \"\"\"Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection \"\"\" from instancelib import InstanceProvider import numpy as np from typing import (Callable, Optional, List, Dict, Tuple, Any, Sequence, FrozenSet, Union) from instancelib.instances.text import TextInstance from instancelib.labels import LabelProvider from instancelib.machinelearning import AbstractClassifier from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_selection import mutual_info_classif from text_explainability.utils import default_tokenizer from text_explainability.default import Readable from text_explainability.generation.return_types import FeatureList from text_explainability.internationalization import translate_list from text_explainability.data.sampling import KMedoids, MMDCritic from text_explainability.data.sampling import LabelwiseKMedoids, LabelwiseMMDCritic class GlobalExplanation(Readable): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = 0 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model: assert model is not None, \\ 'Provide a model to explain its predictions, or set `explain_predictions` to False' else: assert labelprovider is not None, \\ 'Provide a labelprovider to explain ground-truth labels, or set `explain_predictions` to True' instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for id, x in labels] return instances, np.array(labels) class TokenFrequency(GlobalExplanation): def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), tokenizer: Callable = default_tokenizer, **count_vectorizer_kwargs) -> Dict[str, List[Tuple[str, int]]]: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. tokenizer (Callable, optional): [description]. Defaults to default_tokenizer. Returns: Dict[str, List[Tuple[str, int]]]: Each label with corresponding top words and their frequency \"\"\" instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(tokenizer=tokenizer, stop_words=filter_words, max_features=k, **count_vectorizer_kwargs) counts = cv.fit_transform(instances_to_fit) counts = np.array(counts.sum(axis=0)).reshape(-1) return sorted(((k_, counts[v_]) for k_, v_ in cv.vocabulary_.items()), key=lambda x: x[1], reverse=True) if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? return {label: top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]]) for label in np.unique(labels)} return FeatureList('all', top_k_counts(instances.all_data())) class TokenInformation(GlobalExplanation): def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), tokenizer: Callable = default_tokenizer, **count_vectorizer_kwargs) -> List[Tuple[str, float]]: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. tokenizer (Callable, optional): Function for tokenizing strings. Defaults to default_tokenizer. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`. Returns: List[Tuple[str, float]]: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(tokenizer=tokenizer, stop_words=filter_words, **count_vectorizer_kwargs) counts = cv.fit_transform(instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self._seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted(res, key=lambda x: x[1], reverse=True))[:k] return FeatureList(used_features=[a for a, b in res_sorted], scores=[b for a, b in res_sorted]) __all__ = [GlobalExplanation, TokenFrequency, TokenInformation, KMedoids, MMDCritic, LabelwiseKMedoids, LabelwiseMMDCritic]","title":"Global Explanation"},{"location":"reference/text_explainability/global_explanation/#module-text_explainabilityglobal_explanation","text":"Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 \"\"\"Global explanations explain the whole dataset or model behavior on that dataset. Todo: * More support for sampling methods * add support for other tasks than classification (e.g. regression, multi-label classification) * partial dependence plots? https://scikit-learn.org/stable/modules/classes.html#module-sklearn.inspection \"\"\" from instancelib import InstanceProvider import numpy as np from typing import (Callable, Optional, List, Dict, Tuple, Any, Sequence, FrozenSet, Union) from instancelib.instances.text import TextInstance from instancelib.labels import LabelProvider from instancelib.machinelearning import AbstractClassifier from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_selection import mutual_info_classif from text_explainability.utils import default_tokenizer from text_explainability.default import Readable from text_explainability.generation.return_types import FeatureList from text_explainability.internationalization import translate_list from text_explainability.data.sampling import KMedoids, MMDCritic from text_explainability.data.sampling import LabelwiseKMedoids, LabelwiseMMDCritic class GlobalExplanation(Readable): def __init__(self, provider: InstanceProvider[TextInstance, Any, str, Any, str], seed: int = 0): \"\"\"Generic wrapper from global explanations (explain whole dataset or model). Args: provider (InstanceProvider[TextInstance, Any, str, Any, str]): Dataset to perform explanation on. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.provider = provider self._seed = 0 def get_data(self) -> InstanceProvider: \"\"\"Easy access to data. Returns: InstanceProvider: Easily accessible dataset. \"\"\" return self.provider def predict(self, model: AbstractClassifier) -> Union[Sequence[FrozenSet[str]], np.ndarray]: \"\"\"Apply predict function of model to data. Args: model (AbstractClassifier): Model to apply predictions with. Returns: Union[Sequence[FrozenSet[str]], np.ndarray]: Labels for dataset according to model. \"\"\" return model.predict(self.get_data()) def get_instances_labels(self, model: Optional[AbstractClassifier], labelprovider: Optional[LabelProvider], explain_model: bool = True) -> Tuple[InstanceProvider, np.ndarray]: \"\"\"Get corresponding labels of dataset inputs, either from the original data or according to the predict function. Args: model (Optional[AbstractClassifier]): Model to perform predictions with. labelprovider (Optional[LabelProvider]): Ground-truth labels. explain_model (bool, optional): Whether to explain using the `model` labels (True) or `labelprovider` labels (False). Defaults to True. Returns: Tuple[InstanceProvider, np.ndarray]: Instances and corresponding labels \"\"\" if explain_model: assert model is not None, \\ 'Provide a model to explain its predictions, or set `explain_predictions` to False' else: assert labelprovider is not None, \\ 'Provide a labelprovider to explain ground-truth labels, or set `explain_predictions` to True' instances = self.get_data() labels = model.predict(instances) if explain_model \\ else [next(iter(labelprovider.get_labels(k))) for k in instances] if len(labels) > 0 and isinstance(labels[0], tuple) and isinstance(labels[0][-1], frozenset): labels = ['-'.join(list(x)) for id, x in labels] return instances, np.array(labels) class TokenFrequency(GlobalExplanation): def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), tokenizer: Callable = default_tokenizer, **count_vectorizer_kwargs) -> Dict[str, List[Tuple[str, int]]]: \"\"\"Show the top-k number of tokens for each ground-truth or predicted label. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. labelwise (bool, optional): Whether to summarize the counts for each label seperately. Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. tokenizer (Callable, optional): [description]. Defaults to default_tokenizer. Returns: Dict[str, List[Tuple[str, int]]]: Each label with corresponding top words and their frequency \"\"\" instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) def top_k_counts(instances_to_fit): cv = CountVectorizer(tokenizer=tokenizer, stop_words=filter_words, max_features=k, **count_vectorizer_kwargs) counts = cv.fit_transform(instances_to_fit) counts = np.array(counts.sum(axis=0)).reshape(-1) return sorted(((k_, counts[v_]) for k_, v_ in cv.vocabulary_.items()), key=lambda x: x[1], reverse=True) if labelwise: # TO-DO improve beyond classification, e.g. buckets for regression? return {label: top_k_counts([instances[instances.key_list[idx]].data for idx in np.where(labels == label)[0]]) for label in np.unique(labels)} return FeatureList('all', top_k_counts(instances.all_data())) class TokenInformation(GlobalExplanation): def __call__(self, model: Optional[AbstractClassifier] = None, labelprovider: Optional[LabelProvider] = None, explain_model: bool = True, # labelwise: bool = True, k: Optional[int] = None, filter_words: List[str] = translate_list('stopwords'), tokenizer: Callable = default_tokenizer, **count_vectorizer_kwargs) -> List[Tuple[str, float]]: \"\"\"Show the top-k token mutual information for a dataset or model. Args: model (Optional[AbstractClassifier], optional): Predictive model to explain. Defaults to None. labelprovider (Optional[LabelProvider], optional): Ground-truth labels to explain. Defaults to None. explain_model (bool, optional): Whether to explain the model (True) or ground-truth labels (False). Defaults to True. k (Optional[int], optional): Limit to the top-k words per label, or all words if None. Defaults to None. filter_words (List[str], optional): Words to filter out from top-k. Defaults to ['de', 'het', 'een']. tokenizer (Callable, optional): Function for tokenizing strings. Defaults to default_tokenizer. **count_vectorizer_kwargs: Keyword arguments to pass onto `CountVectorizer`. Returns: List[Tuple[str, float]]: k labels, sorted based on their mutual information with the output (predictive model labels or ground-truth labels) \"\"\" instances, labels = self.get_instances_labels(model, labelprovider, explain_model=explain_model) cv = CountVectorizer(tokenizer=tokenizer, stop_words=filter_words, **count_vectorizer_kwargs) counts = cv.fit_transform(instances.all_data()) # TO-DO improve beyond classification # see https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html # #sklearn.feature_selection.mutual_info_regression mif = mutual_info_classif(counts, labels, discrete_features=True, random_state=self._seed) feature_names = cv.get_feature_names() res = list(map(tuple, zip(feature_names, mif))) res_sorted = list(sorted(res, key=lambda x: x[1], reverse=True))[:k] return FeatureList(used_features=[a for a, b in res_sorted], scores=[b for a, b in res_sorted]) __all__ = [GlobalExplanation, TokenFrequency, TokenInformation, KMedoids, MMDCritic, LabelwiseKMedoids, LabelwiseMMDCritic]","title":"Module text_explainability.global_explanation"},{"location":"reference/text_explainability/internationalization/","text":"Module text_explainability.internationalization Support for i18n internationalization. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 \"\"\"Support for i18n internationalization.\"\"\" import os import i18n from typing import List FOLDER = os.path.abspath(os.path.dirname(os.path.abspath(__file__))) if not os.path.isdir(f'{FOLDER}{os.path.sep}locale'): FOLDER = os.path.join(FOLDER, 'text_explainability') i18n.load_path.append(os.path.join(FOLDER, 'locale')) i18n.set('filename_format', '{locale}.{format}') i18n.set('file_format', 'json') i18n.set('locale', 'nl') i18n.set('fallback', 'en') i18n.set('skip_locale_root_data', True) i18n.resource_loader.init_json_loader() def translate_string(id: str) -> str: \"\"\"Get a string based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of string in `lang.{locale}.yml` file. Returns: str: String corresponding to locale. \"\"\" return i18n.t(f'{id}') def translate_list(id: str, sep: str = ';') -> List[str]: \"\"\"Get a list based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of list in `lang.{locale}.yml` file. sep (str, optional): Separator to split elements of list. Defaults to ';'. Returns: List[str]: List corresponding to locale. \"\"\" return i18n.t(f'{id}').split(sep) def set_locale(locale: str) -> None: \"\"\"Set current locale (choose from `en`, `nl`). Args: locale (str): Locale to change to. \"\"\" return i18n.set('locale', locale) def get_locale() -> str: \"\"\"Get current locale. Returns: str: Current locale. \"\"\" return i18n.get('locale') Variables 1 FOLDER Functions get_locale 1 2 3 def get_locale ( ) -> str Get current locale. Returns: Type Description str Current locale. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_locale() -> str: \"\"\"Get current locale. Returns: str: Current locale. \"\"\" return i18n.get('locale') set_locale 1 2 3 def set_locale ( locale : str ) -> None Set current locale (choose from en , nl ). Parameters: Name Type Description Default locale str Locale to change to. None View Source 1 2 3 4 5 6 7 8 9 10 11 def set_locale(locale: str) -> None: \"\"\"Set current locale (choose from `en`, `nl`). Args: locale (str): Locale to change to. \"\"\" return i18n.set('locale', locale) translate_list 1 2 3 4 def translate_list ( id : str , sep : str = ';' ) -> List [ str ] Get a list based on locale , as defined in the './locale' folder. Parameters: Name Type Description Default id str Identifier of list in lang.{locale}.yml file. None sep str Separator to split elements of list. Defaults to ';'. ';' Returns: Type Description List[str] List corresponding to locale. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def translate_list(id: str, sep: str = ';') -> List[str]: \"\"\"Get a list based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of list in `lang.{locale}.yml` file. sep (str, optional): Separator to split elements of list. Defaults to ';'. Returns: List[str]: List corresponding to locale. \"\"\" return i18n.t(f'{id}').split(sep) translate_string 1 2 3 def translate_string ( id : str ) -> str Get a string based on locale , as defined in the './locale' folder. Parameters: Name Type Description Default id str Identifier of string in lang.{locale}.yml file. None Returns: Type Description str String corresponding to locale. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def translate_string(id: str) -> str: \"\"\"Get a string based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of string in `lang.{locale}.yml` file. Returns: str: String corresponding to locale. \"\"\" return i18n.t(f'{id}')","title":"Internationalization"},{"location":"reference/text_explainability/internationalization/#module-text_explainabilityinternationalization","text":"Support for i18n internationalization. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 \"\"\"Support for i18n internationalization.\"\"\" import os import i18n from typing import List FOLDER = os.path.abspath(os.path.dirname(os.path.abspath(__file__))) if not os.path.isdir(f'{FOLDER}{os.path.sep}locale'): FOLDER = os.path.join(FOLDER, 'text_explainability') i18n.load_path.append(os.path.join(FOLDER, 'locale')) i18n.set('filename_format', '{locale}.{format}') i18n.set('file_format', 'json') i18n.set('locale', 'nl') i18n.set('fallback', 'en') i18n.set('skip_locale_root_data', True) i18n.resource_loader.init_json_loader() def translate_string(id: str) -> str: \"\"\"Get a string based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of string in `lang.{locale}.yml` file. Returns: str: String corresponding to locale. \"\"\" return i18n.t(f'{id}') def translate_list(id: str, sep: str = ';') -> List[str]: \"\"\"Get a list based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of list in `lang.{locale}.yml` file. sep (str, optional): Separator to split elements of list. Defaults to ';'. Returns: List[str]: List corresponding to locale. \"\"\" return i18n.t(f'{id}').split(sep) def set_locale(locale: str) -> None: \"\"\"Set current locale (choose from `en`, `nl`). Args: locale (str): Locale to change to. \"\"\" return i18n.set('locale', locale) def get_locale() -> str: \"\"\"Get current locale. Returns: str: Current locale. \"\"\" return i18n.get('locale')","title":"Module text_explainability.internationalization"},{"location":"reference/text_explainability/internationalization/#variables","text":"1 FOLDER","title":"Variables"},{"location":"reference/text_explainability/internationalization/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/internationalization/#get_locale","text":"1 2 3 def get_locale ( ) -> str Get current locale. Returns: Type Description str Current locale. View Source 1 2 3 4 5 6 7 8 9 10 11 def get_locale() -> str: \"\"\"Get current locale. Returns: str: Current locale. \"\"\" return i18n.get('locale')","title":"get_locale"},{"location":"reference/text_explainability/internationalization/#set_locale","text":"1 2 3 def set_locale ( locale : str ) -> None Set current locale (choose from en , nl ). Parameters: Name Type Description Default locale str Locale to change to. None View Source 1 2 3 4 5 6 7 8 9 10 11 def set_locale(locale: str) -> None: \"\"\"Set current locale (choose from `en`, `nl`). Args: locale (str): Locale to change to. \"\"\" return i18n.set('locale', locale)","title":"set_locale"},{"location":"reference/text_explainability/internationalization/#translate_list","text":"1 2 3 4 def translate_list ( id : str , sep : str = ';' ) -> List [ str ] Get a list based on locale , as defined in the './locale' folder. Parameters: Name Type Description Default id str Identifier of list in lang.{locale}.yml file. None sep str Separator to split elements of list. Defaults to ';'. ';' Returns: Type Description List[str] List corresponding to locale. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def translate_list(id: str, sep: str = ';') -> List[str]: \"\"\"Get a list based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of list in `lang.{locale}.yml` file. sep (str, optional): Separator to split elements of list. Defaults to ';'. Returns: List[str]: List corresponding to locale. \"\"\" return i18n.t(f'{id}').split(sep)","title":"translate_list"},{"location":"reference/text_explainability/internationalization/#translate_string","text":"1 2 3 def translate_string ( id : str ) -> str Get a string based on locale , as defined in the './locale' folder. Parameters: Name Type Description Default id str Identifier of string in lang.{locale}.yml file. None Returns: Type Description str String corresponding to locale. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def translate_string(id: str) -> str: \"\"\"Get a string based on `locale`, as defined in the './locale' folder. Args: id (str): Identifier of string in `lang.{locale}.yml` file. Returns: str: String corresponding to locale. \"\"\" return i18n.t(f'{id}')","title":"translate_string"},{"location":"reference/text_explainability/local_explanation/","text":"Module text_explainability.local_explanation Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 \"\"\"Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors \"\"\" import numpy as np import math from typing import Callable, Optional, Sequence, Tuple, Union from instancelib import (AbstractEnvironment, InstanceProvider, LabelProvider, MemoryLabelProvider, TextEnvironment) from instancelib.machinelearning import AbstractClassifier from instancelib.instances.text import TextInstance, TextInstanceProvider from sklearn.linear_model import Ridge from sklearn.tree import DecisionTreeClassifier from text_explainability.data.augmentation import (LocalTokenPertubator, TokenReplacement) from text_explainability.data.weights import (exponential_kernel, pairwise_distances) from text_explainability.default import Readable from text_explainability.generation.feature_selection import FeatureSelector from text_explainability.generation.return_types import FeatureAttribution, Rules from text_explainability.generation.surrogate import (LinearSurrogate, TreeSurrogate, RuleSurrogate) from text_explainability.generation.target_encoding import FactFoilEncoder from text_explainability.decorators import text_instance from text_explainability.utils import binarize, default_detokenizer import six import sys sys.modules['sklearn.externals.six'] = six # ensure backward compatibility from skrules import SkopeRules # noqa: E402 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env class LocalExplanation(Readable): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = TokenReplacement(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment], local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self._seed)) self.local_model = local_model @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): assert self.labelset is not None, 'can only provide label names when such a list exists' labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset) class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # To-do: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset) class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment], labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, rules=self.local_model, labelset=self.labelset, sampled=True) class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, rules=self.local_model, labelset=labelset, sampled=True) class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, rules=self.local_model, labelset=labelset, sampled=True) Functions default_env 1 2 3 def default_env ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None ) -> instancelib . environment . base . AbstractEnvironment If no environment is supplied, an empty Enviroment is created for text data. Parameters: Name Type Description Default env Optional[AbstractEnvironment] If a environment is supplied, it is used, otherwise. None Returns: Type Description AbstractEnvironment The default/supplied environment. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env Classes Anchor 1 2 3 4 5 6 class Anchor ( env : Optional [ instancelib . environment . base . AbstractEnvironment ], labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment], labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable Static methods beam_search 1 2 3 4 5 6 7 8 9 10 11 def beam_search ( provider , perturbed : numpy . ndarray , model , beam_size : int = 1 , min_confidence : float = 0.95 , delta : float = 0.05 , epsilon : float = 0.1 , max_anchor_size : Optional [ int ] = None , batch_size : int = 20 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') dlow_bernoulli 1 2 3 4 def dlow_bernoulli ( p , level ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm kl_bernoulli 1 2 3 4 def kl_bernoulli ( p , q ) View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed best_candidate 1 2 3 def best_candidate ( self ) View Source 1 2 3 def best_candidate(self): pass generate_candidates 1 2 3 def generate_candidates ( self ) View Source 1 2 3 def generate_candidates(self,): pass FactFoilMixin 1 2 3 4 5 class FactFoilMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) Descendants text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Methods to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) FoilTree 1 2 3 4 5 6 7 8 9 10 class FoilTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, rules=self.local_model, labelset=labelset, sampled=True) Ancestors (in MRO) text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) KernelSHAP 1 2 3 4 5 6 class KernelSHAP ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : text_explainability . data . augmentation . LocalTokenPertubator = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # To-do: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable Static methods select_features 1 2 3 4 5 6 def select_features ( X : numpy . ndarray , y : numpy . ndarray , default_features : int = 1 , l1_reg : Union [ int , float , str ] = 'auto' ) -> numpy . ndarray Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either auto , n_features({int}) , {int} , {float} , aic or bic . Defaults to 'auto'. Raises: Exception: Unknown value for l1_reg Returns: np.ndarray: Feature indices to include. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed LIME 1 2 3 4 5 6 7 8 9 class LIME ( env : Optional [ instancelib . environment . base . AbstractEnvironment ], local_model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment], local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self._seed)) self.local_model = local_model @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): assert self.labelset is not None, 'can only provide label names when such a list exists' labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) LocalExplanation 1 2 3 4 5 6 class LocalExplanation ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LocalExplanation(Readable): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = TokenReplacement(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed Ancestors (in MRO) text_explainability.default.Readable Descendants text_explainability.local_explanation.LIME text_explainability.local_explanation.KernelSHAP text_explainability.local_explanation.Anchor text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed LocalRules 1 2 3 4 5 6 7 8 9 10 class LocalRules ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . RuleSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, rules=self.local_model, labelset=labelset, sampled=True) Ancestors (in MRO) text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed to_fact_foil 1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) LocalTree 1 2 3 4 5 6 7 8 9 10 class LocalTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, rules=self.local_model, labelset=self.labelset, sampled=True) Ancestors (in MRO) text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation Methods augment_sample 1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) WeightedExplanation 1 2 3 4 class WeightedExplanation ( kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) Descendants text_explainability.local_explanation.LIME text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules Methods weigh_samples 1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"Local Explanation"},{"location":"reference/text_explainability/local_explanation/#module-text_explainabilitylocal_explanation","text":"Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 \"\"\"Local explanations explain why a model made a prediction for a single instance. Todo: * Implement Anchors \"\"\" import numpy as np import math from typing import Callable, Optional, Sequence, Tuple, Union from instancelib import (AbstractEnvironment, InstanceProvider, LabelProvider, MemoryLabelProvider, TextEnvironment) from instancelib.machinelearning import AbstractClassifier from instancelib.instances.text import TextInstance, TextInstanceProvider from sklearn.linear_model import Ridge from sklearn.tree import DecisionTreeClassifier from text_explainability.data.augmentation import (LocalTokenPertubator, TokenReplacement) from text_explainability.data.weights import (exponential_kernel, pairwise_distances) from text_explainability.default import Readable from text_explainability.generation.feature_selection import FeatureSelector from text_explainability.generation.return_types import FeatureAttribution, Rules from text_explainability.generation.surrogate import (LinearSurrogate, TreeSurrogate, RuleSurrogate) from text_explainability.generation.target_encoding import FactFoilEncoder from text_explainability.decorators import text_instance from text_explainability.utils import binarize, default_detokenizer import six import sys sys.modules['sklearn.externals.six'] = six # ensure backward compatibility from skrules import SkopeRules # noqa: E402 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env class LocalExplanation(Readable): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = TokenReplacement(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric)) class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment], local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self._seed)) self.local_model = local_model @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): assert self.labelset is not None, 'can only provide label names when such a list exists' labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset) class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # To-do: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset) class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment], labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, rules=self.local_model, labelset=self.labelset, sampled=True) class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y) class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, rules=self.local_model, labelset=labelset, sampled=True) class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, rules=self.local_model, labelset=labelset, sampled=True)","title":"Module text_explainability.local_explanation"},{"location":"reference/text_explainability/local_explanation/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/local_explanation/#default_env","text":"1 2 3 def default_env ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None ) -> instancelib . environment . base . AbstractEnvironment If no environment is supplied, an empty Enviroment is created for text data. Parameters: Name Type Description Default env Optional[AbstractEnvironment] If a environment is supplied, it is used, otherwise. None Returns: Type Description AbstractEnvironment The default/supplied environment. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def default_env(env: Optional[AbstractEnvironment] = None) -> AbstractEnvironment: \"\"\"If no environment is supplied, an empty Enviroment is created for text data. Args: env (Optional[AbstractEnvironment], optional): If a environment is supplied, it is used, otherwise. Returns: AbstractEnvironment: The default/supplied environment. \"\"\" if env is not None: return env empty_dataset = TextInstanceProvider([]) empty_labels = MemoryLabelProvider([], {}) empty_env = TextEnvironment(empty_dataset, empty_labels) return empty_env","title":"default_env"},{"location":"reference/text_explainability/local_explanation/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/local_explanation/#anchor","text":"1 2 3 4 5 6 class Anchor ( env : Optional [ instancelib . environment . base . AbstractEnvironment ], labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class Anchor(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment], labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, seed: int = 0): super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))) @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm def generate_candidates(self,): pass def best_candidate(self): pass @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py') @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 100, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None): raise NotImplementedError('Only partially implemented') # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_text.py # https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py provider, perturbed = self.augment_sample(sample, None, sequential=False, contiguous=False, n_samples=n_samples, predict=False) perturbed = binarize(perturbed[1:]) # flatten all n replacements into one y_true = np.argmax(model.predict_proba([provider[0]])[0][-1]) # noqa: F841 # Use beam from https://homes.cs.washington.edu/~marcotcr/aaai18.pdf (Algorithm 2) anchor = Anchor.beam_search(provider, # noqa: F841 perturbed, model, beam_size=beam_size, min_confidence=min_confidence, delta=delta, epsilon=epsilon, max_anchor_size=max_anchor_size, batch_size=n_samples // 10 if n_samples >= 1000 else n_samples // 5) pass","title":"Anchor"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro","text":"text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/local_explanation/#beam_search","text":"1 2 3 4 5 6 7 8 9 10 11 def beam_search ( provider , perturbed : numpy . ndarray , model , beam_size : int = 1 , min_confidence : float = 0.95 , delta : float = 0.05 , epsilon : float = 0.1 , max_anchor_size : Optional [ int ] = None , batch_size : int = 20 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @staticmethod def beam_search(provider, perturbed: np.ndarray, model, beam_size: int = 1, min_confidence: float = 0.95, delta: float = 0.05, epsilon: float = 0.1, max_anchor_size: Optional[int] = None, batch_size: int = 20): assert beam_size >= 1, f'beam size should be at least 1, but is {beam_size}' assert 0.0 <= min_confidence <= 0.95, f'min_confidence should be a value in [0, 1], but is {min_confidence}' assert 0.0 <= delta <= 0.95, f'delta should be a value in [0, 1], but is {delta}' assert 0.0 <= epsilon <= 0.95, f'epsilon should be a value in [0, 1], but is {epsilon}' assert batch_size > 2, 'requires positive batch size' for batch in provider.instance_chunker(batch_size): y = list(model.predict_proba_raw(batch))[-1][-1] # todo: only look at probs of one class y_true, y = y[0], y[1:] # noqa: F841 beta = np.log(1.0 / delta) mean = y.mean() lb = Anchor.dlow_bernoulli(mean, beta / perturbed.shape[0]) if not(mean > min_confidence and lb < min_confidence - epsilon): break raise NotImplementedError('[WIP] Implementing anchor/anchor_base.py')","title":"beam_search"},{"location":"reference/text_explainability/local_explanation/#dlow_bernoulli","text":"1 2 3 4 def dlow_bernoulli ( p , level ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 @staticmethod def dlow_bernoulli(p, level): lm = max(min(1, p - np.sqrt(level / 2.0)), 0.0) qm = (p + lm) / 2.0 if Anchor.kl_bernoulli(p, qm) > level: lm = qm return lm","title":"dlow_bernoulli"},{"location":"reference/text_explainability/local_explanation/#kl_bernoulli","text":"1 2 3 4 def kl_bernoulli ( p , q ) View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def kl_bernoulli(p, q): p = float(min(1 - 1e-15, max(1e-15, p))) q = float(min(1 - 1e-15, max(1e-15, q))) return (p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q)))","title":"kl_bernoulli"},{"location":"reference/text_explainability/local_explanation/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#best_candidate","text":"1 2 3 def best_candidate ( self ) View Source 1 2 3 def best_candidate(self): pass","title":"best_candidate"},{"location":"reference/text_explainability/local_explanation/#generate_candidates","text":"1 2 3 def generate_candidates ( self ) View Source 1 2 3 def generate_candidates(self,): pass","title":"generate_candidates"},{"location":"reference/text_explainability/local_explanation/#factfoilmixin","text":"1 2 3 4 5 class FactFoilMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 class FactFoilMixin: def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"FactFoilMixin"},{"location":"reference/text_explainability/local_explanation/#descendants","text":"text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#foiltree","text":"1 2 3 4 5 6 7 8 9 10 class FoilTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class FoilTree(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y_, weights=weights) # TODO: pass to which label the Foil Tree applies return Rules(provider=provider, rules=self.local_model, labelset=labelset, sampled=True)","title":"FoilTree"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_1","text":"text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_1","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil_1","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#weigh_samples","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#kernelshap","text":"1 2 3 4 5 6 class KernelSHAP ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : text_explainability . data . augmentation . LocalTokenPertubator = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 class KernelSHAP(LocalExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: LocalTokenPertubator = None, seed: int = 0): \"\"\"Calculates `Shapley values`_ for an instance to explain, assuming the model is a black-box. Calculates Shapley values (local, additive feature attribution scores) for an instance to explain, by calculating the average contribution of changing combinations of feature values. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _Shapley values: https://github.com/slundberg/shap \"\"\" super().__init__(env=env, augmenter=augmenter, labelset=labelset, seed=seed) @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: Optional[int] = None, l1_reg: Union[int, float, str] = 'auto') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `KernelShap`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. n_samples (Optional[int], optional): Number of neighborhood samples to generate (if None defaults to `2 * sample_len + 2 ** 11`). Defaults to None. l1_reg (Union[int, float, str], optional): Method for regularization (limiting number of features), either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Returns: FeatureAttribution: [description] .. _KernelShap: https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py \"\"\" sample_len = len(sample.tokenized) if n_samples is None: n_samples = 2 * sample_len + 2 ** 11 n_samples = min(n_samples, 2 ** 30) provider, perturbed, y = self.augment_sample(sample, model, sequential=True, contiguous=False, n_samples=n_samples, add_background_instance=True) # To-do: exclude non-varying feature groups y_null, y = y[-1], y[1:-1] y -= y_null used_features = np.arange(perturbed.shape[1]) phi = np.zeros([sample_len, y.shape[1]]) phi_var = np.zeros(sample_len) if perturbed.shape[1] == 1: phi = np.mean(y - y_null, axis=0).reshape(1, -1) elif perturbed.shape[1] > 1: # Weigh samples M = perturbed.shape[1] Z = np.sum(perturbed[1:-1], axis=1).astype(int) weight_vector = np.array([(M - 1) / (math.comb(M, m) * m * (M - m)) for m in range(1, M)]) weight_vector /= np.sum(weight_vector) kernel_weights = weight_vector[Z - 1] nonzero = KernelSHAP.select_features(perturbed[1:-1], y, default_features=sample_len, l1_reg=l1_reg) used_features = nonzero phi_var = np.ones(sample_len) if len(used_features) > 0: X = perturbed[1:-1] X_W = np.dot(X.T, np.diag(kernel_weights)) try: tmp2 = np.linalg.inv(np.dot(X_W, X)) except np.linalg.LinAlgError: tmp2 = np.linalg.pinv(np.dot(X_W, X)) phi = np.dot(tmp2, np.dot(X_W, y)).T return FeatureAttribution(provider=provider, scores=phi, scores_stddev=phi_var, base_score=y_null, used_features=used_features, labels=np.arange(y.shape[1]), labelset=self.labelset)","title":"KernelSHAP"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_2","text":"text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/text_explainability/local_explanation/#select_features","text":"1 2 3 4 5 6 def select_features ( X : numpy . ndarray , y : numpy . ndarray , default_features : int = 1 , l1_reg : Union [ int , float , str ] = 'auto' ) -> numpy . ndarray Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either auto , n_features({int}) , {int} , {float} , aic or bic . Defaults to 'auto'. Raises: Exception: Unknown value for l1_reg Returns: np.ndarray: Feature indices to include. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @staticmethod def select_features(X: np.ndarray, y: np.ndarray, default_features: int = 1, l1_reg: Union[int, float, str] = 'auto') -> np.ndarray: \"\"\"Select features for data X and corresponding output y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. default_features (int, optional): Default number of features, when returning all features. Defaults to 1. l1_reg (Union[int, float, str], optional): Method for regularization, either `auto`, `n_features({int})`, `{int}`, `{float}`, `aic` or `bic`. Defaults to 'auto'. Raises: Exception: Unknown value for `l1_reg` Returns: np.ndarray: Feature indices to include. \"\"\" feature_selector = FeatureSelector() nonzero = np.arange(default_features) if isinstance(l1_reg, str) and l1_reg.startswith('n_features('): l1_reg = int(l1_reg[len('n_features('):-1]) if isinstance(l1_reg, int): nonzero = feature_selector(X, y, n_features=l1_reg, method='l1_reg') elif isinstance(l1_reg, float): nonzero = feature_selector(X, y, method='l1_reg', alpha=l1_reg) elif l1_reg in ['auto', 'aic', 'bic']: if l1_reg == 'auto': l1_reg = 'aic' nonzero = feature_selector(X, y, method=l1_reg) else: raise Exception(f'Unknown value \"{l1_reg}\" for l1_reg') return nonzero","title":"select_features"},{"location":"reference/text_explainability/local_explanation/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_2","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#lime","text":"1 2 3 4 5 6 7 8 9 class LIME ( env : Optional [ instancelib . environment . base . AbstractEnvironment ], local_model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class LIME(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment], local_model: Optional[LinearSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Local Interpretable Model-Agnostic Explanations (`LIME`_). Implementation of local linear surrogate model on (weighted) perturbed text data, to get feature attribution scores for an example instance. Args: env (Optional[AbstractEnvironment]): Environment to save local perturbations in. Defaults to None. local_model (Optional[LinearSurrogate], optional): Local linear model. Defaults to None. kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance. Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _LIME: https://github.com/marcotcr/lime \"\"\" LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = LinearSurrogate(Ridge(alpha=1, fit_intercept=True, random_state=self._seed)) self.local_model = local_model @text_instance(tokenize=True) def __call__(self, sample: TextInstance, model: AbstractClassifier, labels: Optional[Union[Sequence[int], Sequence[str]]] = None, n_samples: int = 50, n_features: int = 10, feature_selection_method: str = 'auto', weigh_samples: bool = True, distance_metric: str = 'cosine') -> FeatureAttribution: \"\"\"Calculate feature attribution scores using `LIME Text`_. Args: sample (TextInstance): Instance to explain. model (AbstractClassifier): Model to explain. labels (Optional[Union[Sequence[int], Sequence[str]]], optional): [description]. Defaults to None. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. n_features (int, optional): Maximum number of features to include (explanation length). Defaults to 10. feature_selection_method (str, optional): Method for limiting number of features, either `forward_selection`, `highest_weights` or `auto`. Defaults to 'auto'. weigh_samples (bool, optional): Whether to locally weigh samples based on their similarity to the original instance. Defaults to True. distance_metric (str, optional): Distance metric for local weighting. Defaults to 'cosine'. Returns: FeatureAttribution: [description] .. _LIME Text: https://github.com/marcotcr/lime/blob/master/lime/lime_text.py \"\"\" if labels is not None: if isinstance(labels, (int, str)): labels = [labels] n_labels = sum(1 for _ in iter(labels)) if n_labels > 0 and isinstance(next(iter(labels)), str): assert self.labelset is not None, 'can only provide label names when such a list exists' labels = [self.labelset.index(label) for label in labels] # Generate neighborhood samples provider, perturbed, y = self.augment_sample(sample, model, sequential=False, contiguous=False, n_samples=n_samples) perturbed = binarize(perturbed) # flatten all n replacements into one if weigh_samples: weights = self.weigh_samples(perturbed, metric=distance_metric) if feature_selection_method == 'auto': feature_selection_method = 'forward_selection' if n_features <= 6 else 'highest_weights' feature_importances, used_features = [], {} if labels is None: labels = np.arange(y.shape[1]) for label in labels: # Look at output for label y_label = y[:, label].copy() # Get the most important features features = FeatureSelector(self.local_model)(perturbed, y_label, weights=weights, n_features=n_features, method=feature_selection_method) # Fit explanation model self.local_model.alpha_reset() self.local_model.fit(perturbed[:, features], y_label, weights=weights) feature_importances.append(self.local_model.feature_importances) used_features[label] = features return FeatureAttribution(provider=provider, scores=feature_importances, used_features=used_features, labels=labels, labelset=self.labelset)","title":"LIME"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_3","text":"text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_3","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_1","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#localexplanation","text":"1 2 3 4 5 6 class LocalExplanation ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LocalExplanation(Readable): def __init__(self, env: Optional[AbstractEnvironment] = None, augmenter: Optional[LocalTokenPertubator] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, seed: int = 0): \"\"\"Generate explanation for a single decision. Args: env (Optional[AbstractEnvironment], optional): Environment to save local perturbations in. Defaults to None. augmenter (Optional[LocalTokenPertubator], optional): Function to augment data with perturbations, to generate neighborhood data. Defaults to None. labelset (Optional[Union[Sequence[str], LabelProvider]], optional): Sequence of label names or LabelProvider containing named labels. When not supplied, it uses identifiers for labels. Defaults to None. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__() self.env = default_env(env) if augmenter is None: augmenter = TokenReplacement(env=self.env, detokenizer=default_detokenizer) if isinstance(labelset, LabelProvider) and hasattr(labelset, 'labelset'): labelset = list(labelset.labelset) elif labelset is None and self.env is not None: if hasattr(self.env.labels, 'labelset'): labelset = list(self.env.labels.labelset) self.labelset = labelset self.augmenter = augmenter self._seed = seed @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"LocalExplanation"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_4","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#descendants_1","text":"text_explainability.local_explanation.LIME text_explainability.local_explanation.KernelSHAP text_explainability.local_explanation.Anchor text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#methods_5","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_4","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#localrules","text":"1 2 3 4 5 6 7 8 9 10 class LocalRules ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . RuleSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class LocalRules(FactFoilMixin, LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[RuleSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = RuleSurrogate(SkopeRules(max_depth_duplication=2, n_estimators=30, random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, foil_fn: Union[FactFoilEncoder, int, str], n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one # Encode foil as 0 and rest as 1 labelset = self.labelset if self.labelset else model y_ = self.to_fact_foil(y, labelset, foil_fn) weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.fit(perturbed, y_, weights=weights) return Rules(provider, rules=self.local_model, labelset=labelset, sampled=True)","title":"LocalRules"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_5","text":"text_explainability.local_explanation.FactFoilMixin text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#methods_6","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_5","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#to_fact_foil_2","text":"1 2 3 4 5 6 def to_fact_foil ( self , y , labelset , foil_fn : Union [ text_explainability . generation . target_encoding . FactFoilEncoder , int , str ] ) View Source 1 2 3 4 5 6 7 8 9 10 11 def to_fact_foil(self, y, labelset, foil_fn: Union[FactFoilEncoder, int, str]): if isinstance(foil_fn, str): foil_fn = FactFoilEncoder.from_str(foil_fn, labelset) elif isinstance(foil_fn, int): foil_fn = FactFoilEncoder(foil_fn, labelset) return foil_fn(y)","title":"to_fact_foil"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_2","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#localtree","text":"1 2 3 4 5 6 7 8 9 10 class LocalTree ( env : Optional [ instancelib . environment . base . AbstractEnvironment ] = None , labelset : Union [ Sequence [ str ], instancelib . labels . base . LabelProvider , NoneType ] = None , augmenter : Optional [ text_explainability . data . augmentation . LocalTokenPertubator ] = None , local_model : Optional [ text_explainability . generation . surrogate . TreeSurrogate ] = None , kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 , explanation_type : str = 'multiclass' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class LocalTree(LocalExplanation, WeightedExplanation): def __init__(self, env: Optional[AbstractEnvironment] = None, labelset: Optional[Union[Sequence[str], LabelProvider]] = None, augmenter: Optional[LocalTokenPertubator] = None, local_model: Optional[TreeSurrogate] = None, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25, explanation_type: str = 'multiclass', seed: int = 0): LocalExplanation.__init__(self, env=env, augmenter=augmenter, labelset=labelset, seed=seed) WeightedExplanation.__init__(self, kernel=kernel, kernel_width=kernel_width) if local_model is None: local_model = TreeSurrogate(DecisionTreeClassifier(random_state=self._seed)) self.local_model = local_model self.explanation_type = explanation_type @text_instance def __call__(self, sample: TextInstance, model: AbstractClassifier, n_samples: int = 50, weigh_samples: bool = True, distance_metric: str = 'cosine', max_rule_size: Optional[int] = None, **sample_kwargs): provider, perturbed, y = self.augment_sample(sample, model, n_samples=n_samples, avoid_proba=True, **sample_kwargs) perturbed = binarize(perturbed) # flatten all n replacements into one weights = self.weigh_samples(perturbed, metric=distance_metric) if weigh_samples else None self.local_model.max_rule_size = max_rule_size self.local_model.fit(perturbed, y, weights=weights) return Rules(provider=provider, rules=self.local_model, labelset=self.labelset, sampled=True)","title":"LocalTree"},{"location":"reference/text_explainability/local_explanation/#ancestors-in-mro_6","text":"text_explainability.local_explanation.LocalExplanation text_explainability.default.Readable text_explainability.local_explanation.WeightedExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/local_explanation/#methods_7","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#augment_sample_6","text":"1 2 3 4 5 6 7 8 9 10 11 def augment_sample ( self , sample : instancelib . instances . text . TextInstance , model : instancelib . machinelearning . base . AbstractClassifier , sequential : bool = False , contiguous : bool = False , n_samples : int = 50 , add_background_instance : bool = False , predict : bool = True , avoid_proba : bool = False ) -> Union [ Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray ], Tuple [ instancelib . instances . base . InstanceProvider , numpy . ndarray , numpy . ndarray ]] Augment a single sample to generate neighborhood data. Parameters: Name Type Description Default sample TextInstance Instance to perturb. None model AbstractClassifier Model to provide predictions for neighborhood data. None sequential bool Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. False contiguous bool Whether to apply perturbations on contiguous stretches of text. Defaults to False. False n_samples int Number of neighborhood samples to generate. Defaults to 50. 50 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. False predict bool Defaults to True. None avoid_proba bool Model predictions als labels (True) or probabilities when available (False). Defaults to False. None Returns: Type Description Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]] Provider, how instances were perturbed and optionally the corresponding predictions for each instance. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @text_instance(tokenize=True) def augment_sample(self, sample: TextInstance, model: AbstractClassifier, sequential: bool = False, contiguous: bool = False, n_samples: int = 50, add_background_instance: bool = False, predict: bool = True, avoid_proba: bool = False ) -> Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: \"\"\"Augment a single sample to generate neighborhood data. Args: sample (TextInstance): Instance to perturb. model (AbstractClassifier): Model to provide predictions for neighborhood data. sequential (bool, optional): Whether to sequentially sample based on length (first length 1, then 2, ...). Defaults to False. contiguous (bool, optional): Whether to apply perturbations on contiguous stretches of text. Defaults to False. n_samples (int, optional): Number of neighborhood samples to generate. Defaults to 50. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. predict (bool, optional): Defaults to True. avoid_proba (bool, optional): Model predictions als labels (True) or probabilities when available (False). Defaults to False. Returns: Union[Tuple[InstanceProvider, np.ndarray], Tuple[InstanceProvider, np.ndarray, np.ndarray]]: Provider, how instances were perturbed and optionally the corresponding predictions for each instance. \"\"\" provider = self.env.create_empty_provider() sample.map_to_original = np.ones(len(sample.tokenized), dtype=int) provider.add(sample) # Do sampling augmenter = self.augmenter(sample, sequential=sequential, contiguous=contiguous, n_samples=n_samples, add_background_instance=add_background_instance) for perturbed_sample in augmenter.bulk_get_all(): provider.add(perturbed_sample) # Perform prediction if predict: ys = model.predict_proba_raw(provider) y = np.vstack([y_ for id, y_ in ys]).squeeze() if avoid_proba: y = np.argmax(y, axis=1) # Mapping to which instances were perturbed perturbed = np.stack([instance.map_to_original for instance in provider.get_all()]) if predict: return provider, perturbed, y return provider, perturbed","title":"augment_sample"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_3","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/local_explanation/#weightedexplanation","text":"1 2 3 4 class WeightedExplanation ( kernel : Optional [ Callable ] = None , kernel_width : Union [ int , float ] = 25 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class WeightedExplanation: def __init__(self, kernel: Optional[Callable] = None, kernel_width: Union[int, float] = 25): \"\"\"Add weights to neighborhood data. Args: kernel (Optional[Callable], optional): Kernel to determine similarity of perturbed instances to original instance (if set to None defaults to `data.weights.exponential_kernel`). Defaults to None. kernel_width (Union[int, float], optional): Hyperparameter for similarity function of kernel. Defaults to 25. \"\"\" if kernel is None: kernel = exponential_kernel self.kernel_fn = lambda d: kernel(d, kernel_width) def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"WeightedExplanation"},{"location":"reference/text_explainability/local_explanation/#descendants_2","text":"text_explainability.local_explanation.LIME text_explainability.local_explanation.LocalTree text_explainability.local_explanation.FoilTree text_explainability.local_explanation.LocalRules","title":"Descendants"},{"location":"reference/text_explainability/local_explanation/#methods_8","text":"","title":"Methods"},{"location":"reference/text_explainability/local_explanation/#weigh_samples_4","text":"1 2 3 4 5 6 def weigh_samples ( self , a , b = None , metric = 'cosine' ) View Source 1 2 3 4 5 6 7 def weigh_samples(self, a, b=None, metric='cosine'): if b is None: b = a[0] return self.kernel_fn(pairwise_distances(a, b, metric=metric))","title":"weigh_samples"},{"location":"reference/text_explainability/locale/","text":"Module text_explainability.locale This folder contains JSON files with language-specific strings and translations. None View Source 1 \"\"\"This folder contains JSON files with language-specific strings and translations.\"\"\"","title":"Locale"},{"location":"reference/text_explainability/locale/#module-text_explainabilitylocale","text":"This folder contains JSON files with language-specific strings and translations. None View Source 1 \"\"\"This folder contains JSON files with language-specific strings and translations.\"\"\"","title":"Module text_explainability.locale"},{"location":"reference/text_explainability/utils/","text":"Module text_explainability.utils Utility functions. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \"\"\"Utility functions.\"\"\" import re import string import numpy as np from typing import Sequence, Iterable def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input) def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) def binarize(X: np.ndarray): return (X > 0).astype(int) default_tokenizer = word_tokenizer default_detokenizer = word_detokenizer PUNCTUATION = list(string.punctuation) + ['...'] Variables 1 PUNCTUATION Functions binarize 1 2 3 def binarize ( X : numpy . ndarray ) View Source 1 2 3 def binarize(X: np.ndarray): return (X > 0).astype(int) character_detokenizer 1 2 3 def character_detokenizer ( input : Iterable [ str ] ) -> str Convert a list of characters into a string. View Source 1 2 3 4 5 def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) character_tokenizer 1 2 3 def character_tokenizer ( input : str ) -> Sequence [ str ] Convert a string into a list of characters. View Source 1 2 3 4 5 def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) default_detokenizer 1 2 3 def default_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() default_tokenizer 1 2 3 def default_tokenizer ( input : str ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input) word_detokenizer 1 2 3 def word_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() word_tokenizer 1 2 3 def word_tokenizer ( input : str ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input)","title":"Utils"},{"location":"reference/text_explainability/utils/#module-text_explainabilityutils","text":"Utility functions. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \"\"\"Utility functions.\"\"\" import re import string import numpy as np from typing import Sequence, Iterable def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input) def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip() def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input) def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input) def binarize(X: np.ndarray): return (X > 0).astype(int) default_tokenizer = word_tokenizer default_detokenizer = word_detokenizer PUNCTUATION = list(string.punctuation) + ['...']","title":"Module text_explainability.utils"},{"location":"reference/text_explainability/utils/#variables","text":"1 PUNCTUATION","title":"Variables"},{"location":"reference/text_explainability/utils/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/utils/#binarize","text":"1 2 3 def binarize ( X : numpy . ndarray ) View Source 1 2 3 def binarize(X: np.ndarray): return (X > 0).astype(int)","title":"binarize"},{"location":"reference/text_explainability/utils/#character_detokenizer","text":"1 2 3 def character_detokenizer ( input : Iterable [ str ] ) -> str Convert a list of characters into a string. View Source 1 2 3 4 5 def character_detokenizer(input: Iterable[str]) -> str: \"\"\"Convert a list of characters into a string.\"\"\" return ''.join(input)","title":"character_detokenizer"},{"location":"reference/text_explainability/utils/#character_tokenizer","text":"1 2 3 def character_tokenizer ( input : str ) -> Sequence [ str ] Convert a string into a list of characters. View Source 1 2 3 4 5 def character_tokenizer(input: str) -> Sequence[str]: \"\"\"Convert a string into a list of characters.\"\"\" return list(input)","title":"character_tokenizer"},{"location":"reference/text_explainability/utils/#default_detokenizer","text":"1 2 3 def default_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip()","title":"default_detokenizer"},{"location":"reference/text_explainability/utils/#default_tokenizer","text":"1 2 3 def default_tokenizer ( input : str ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input)","title":"default_tokenizer"},{"location":"reference/text_explainability/utils/#word_detokenizer","text":"1 2 3 def word_detokenizer ( input : Iterable [ str ] ) -> str Simple regex detokenizer, ideally resulting in i = detokenizer(tokenizer(i)) . View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def word_detokenizer(input: Iterable[str]) -> str: \"\"\"Simple regex detokenizer, ideally resulting in `i = detokenizer(tokenizer(i))`.\"\"\" out = \" \".join(input).replace(\"`` \", '\"') \\ .replace(\" ''\", '\"') \\ .replace('. . .', '...') \\ .replace(\" ( \", \" (\") \\ .replace(\" ) \", \") \") out = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", out) out = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", out) out = re.sub(r'(\\s+[0-9]+):\\s+([0-9]+\\s+)', r\"\\1:\\2\", out) return out.replace(\" ` \", \" '\").strip()","title":"word_detokenizer"},{"location":"reference/text_explainability/utils/#word_tokenizer","text":"1 2 3 def word_tokenizer ( input : str ) -> Sequence [ str ] Simple regex tokenizer. View Source 1 2 3 4 5 def word_tokenizer(input: str) -> Sequence[str]: \"\"\"Simple regex tokenizer.\"\"\" return re.findall(r\"\\w+|[^\\w\\s]+\", input)","title":"word_tokenizer"},{"location":"reference/text_explainability/data/","text":"Module text_explainability.data Data sampling and generation. None View Source 1 \"\"\"Data sampling and generation.\"\"\" Sub-modules text_explainability.data.augmentation text_explainability.data.embedding text_explainability.data.sampling text_explainability.data.weights","title":"Index"},{"location":"reference/text_explainability/data/#module-text_explainabilitydata","text":"Data sampling and generation. None View Source 1 \"\"\"Data sampling and generation.\"\"\"","title":"Module text_explainability.data"},{"location":"reference/text_explainability/data/#sub-modules","text":"text_explainability.data.augmentation text_explainability.data.embedding text_explainability.data.sampling text_explainability.data.weights","title":"Sub-modules"},{"location":"reference/text_explainability/data/augmentation/","text":"Module text_explainability.data.augmentation Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 \"\"\"Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] \"\"\" from instancelib.environment.base import AbstractEnvironment from instancelib.environment.text import TextEnvironment from instancelib.instances.base import InstanceProvider import numpy as np import math import itertools from typing import (Callable, Iterable, Any, Iterator, Tuple, Optional, List, Union) from instancelib.instances.text import TextInstance from instancelib.pertubations.base import MultiplePertubator, ChildGenerator from text_explainability.default import Readable from text_explainability.decorators import text_instance from text_explainability.utils import default_detokenizer class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" assert hasattr(instance, 'tokenized'), 'Tokenize your instance before applying a perturbation' if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) class TokenReplacement(LocalTokenPertubator): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) assert replacement_len >= instance_len, \\ f'Too few replacements in `self.replacement`, got {replacement_len} and expected {instance_len}' return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed) Classes LeaveOut 1 2 3 4 5 class LeaveOut ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed) Ancestors (in MRO) text_explainability.data.augmentation.TokenReplacement text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child) LocalTokenPertubator 1 2 3 4 class LocalTokenPertubator ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" assert hasattr(instance, 'tokenized'), 'Tokenize your instance before applying a perturbation' if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) Ancestors (in MRO) instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable Descendants text_explainability.data.augmentation.TokenReplacement Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 def perturb ( self , tokenized_instance : Iterable [ str ], * args : Any , ** kwargs : Any ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] View Source 1 2 3 4 5 def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child) TokenReplacement 1 2 3 4 5 6 class TokenReplacement ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > , replacement : Union [ str , List [ str ], NoneType ] = 'UNKWRDZ' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class TokenReplacement(LocalTokenPertubator): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) assert replacement_len >= instance_len, \\ f'Too few replacements in `self.replacement`, got {replacement_len} and expected {instance_len}' return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive Ancestors (in MRO) text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable Descendants text_explainability.data.augmentation.LeaveOut Static methods binary_inactive 1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res Methods discard_children 1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) get_children 1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) perturb 1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive register_child 1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child)","title":"Augmentation"},{"location":"reference/text_explainability/data/augmentation/#module-text_explainabilitydataaugmentation","text":"Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 \"\"\"Augment a single instance to generate neighborhood data. Todo: * Add more complex sampling methods (e.g. top-k replacement by contextual language model, WordNet, ...) * Replacement with k tokens at each index * Ensure inactive[i] is set to 0 if the replacement token is the same as the original token[i] \"\"\" from instancelib.environment.base import AbstractEnvironment from instancelib.environment.text import TextEnvironment from instancelib.instances.base import InstanceProvider import numpy as np import math import itertools from typing import (Callable, Iterable, Any, Iterator, Tuple, Optional, List, Union) from instancelib.instances.text import TextInstance from instancelib.pertubations.base import MultiplePertubator, ChildGenerator from text_explainability.default import Readable from text_explainability.decorators import text_instance from text_explainability.utils import default_detokenizer class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" assert hasattr(instance, 'tokenized'), 'Tokenize your instance before applying a perturbation' if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance) class TokenReplacement(LocalTokenPertubator): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) assert replacement_len >= instance_len, \\ f'Too few replacements in `self.replacement`, got {replacement_len} and expected {instance_len}' return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed)","title":"Module text_explainability.data.augmentation"},{"location":"reference/text_explainability/data/augmentation/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/augmentation/#leaveout","text":"1 2 3 4 5 class LeaveOut ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class LeaveOut(TokenReplacement): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, seed: int = 0): \"\"\"Leave tokens out of the tokenized sequence. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer, replacement=None, seed=seed)","title":"LeaveOut"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro","text":"text_explainability.data.augmentation.TokenReplacement text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb","text":"1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/augmentation/#localtokenpertubator","text":"1 2 3 4 class LocalTokenPertubator ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class LocalTokenPertubator(MultiplePertubator[TextInstance], ChildGenerator[TextInstance], Readable): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer): \"\"\"Perturb a single instance into neighborhood samples. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. \"\"\" super().__init__() if env is None: env = TextEnvironment.from_data([], [], [], [], []) self.env = env self.detokenizer = detokenizer @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent) def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent) @text_instance(tokenize=True) def __call__(self, instance: TextInstance, discard_children: bool = True, *args, **kwargs) -> Iterator[TextInstance]: \"\"\"Apply perturbations to an instance to generate neighborhood data. Args: instance (TextInstance): Tokenized instance to perturb. discard_children (bool, optional): Remove children from previous passes. Defaults to True. *args: Arguments to be passed on to `perturb()` function. **kwargs: Keyword arguments to be passed on to `perturb()` function. Yields: Iterator[Sequence[TextInstance]]: Neighborhood data instances. \"\"\" assert hasattr(instance, 'tokenized'), 'Tokenize your instance before applying a perturbation' if instance.data not in self.env.all_instances.all_data(): provider = self.env.create_empty_provider() provider.add(instance) if discard_children: self.discard_children(instance) for new_tokenized, map_to_original in self.perturb(instance.tokenized, *args, **kwargs): new_data = self.detokenizer(new_tokenized) new_instance = self.env.create( data=new_data, vector=None, map_to_original=map_to_original, representation=new_data, tokenized=new_tokenized ) self.register_child(instance, new_instance) return self.get_children(instance)","title":"LocalTokenPertubator"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro_1","text":"instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#descendants","text":"text_explainability.data.augmentation.TokenReplacement","title":"Descendants"},{"location":"reference/text_explainability/data/augmentation/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive_1","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children_1","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children_1","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb_1","text":"1 2 3 4 5 6 def perturb ( self , tokenized_instance : Iterable [ str ], * args : Any , ** kwargs : Any ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] View Source 1 2 3 4 5 def perturb(self, tokenized_instance: Iterable[str], *args: Any, **kwargs: Any) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: raise NotImplementedError","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child_1","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/augmentation/#tokenreplacement","text":"1 2 3 4 5 6 class TokenReplacement ( env : Optional [ instancelib . environment . base . AbstractEnvironment [ instancelib . instances . text . TextInstance , Any , Any , Any , Any , Any ]] = None , detokenizer : Optional [ Callable [[ Iterable [ str ]], str ]] = < function word_detokenizer at 0x16ddd3f70 > , replacement : Union [ str , List [ str ], NoneType ] = 'UNKWRDZ' , seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class TokenReplacement(LocalTokenPertubator): def __init__(self, env: Optional[AbstractEnvironment[TextInstance, Any, Any, Any, Any, Any]] = None, detokenizer: Optional[Callable[[Iterable[str]], str]] = default_detokenizer, replacement: Optional[Union[str, List[str]]] = 'UNKWRDZ', seed: int = 0): \"\"\"Perturb a tokenized instance by replacing with a set token (e.g. 'UNKWRDZ') or deleting it. Args: detokenizer (Callable[[Iterable[str]], str]): Mapping back from a tokenized instance to a string used in a predictor. replacement (Optional[Union[str, List[str]]], optional): Replacement string, or set to None if you want to delete the word entirely. Defaults to 'UNKWRDZ'. seed (int, optional): Seed for reproducibility. Defaults to 0. \"\"\" super().__init__(env=env, detokenizer=detokenizer) self.replacement = replacement self._seed = seed def _replace(self, tokenized_instance: Iterable[str], keep: Iterable[int]) -> Iterable[str]: \"\"\"Apply replacement/deletion to tokenized instance. Args: tokenized_instance (Iterable[str]): Tokenized instance. keep (Iterable[int]): Binary indicator whether to keep (1) or replace (0) a token. Returns: Iterable[str]: Tokenized instance with perturbation applied. \"\"\" if not self.replacement or self.replacement is None: return [token for token, i in zip(tokenized_instance, keep) if i == 1] if isinstance(self.replacement, list): instance_len = sum(1 for _ in tokenized_instance) replacement_len = len(self.replacement) assert replacement_len >= instance_len, \\ f'Too few replacements in `self.replacement`, got {replacement_len} and expected {instance_len}' return [self.replacement[i] if j == 0 else token for i, (token, j) in enumerate(zip(tokenized_instance, keep))] return [self.replacement if i == 0 else token for token, i in zip(tokenized_instance, keep)] def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"TokenReplacement"},{"location":"reference/text_explainability/data/augmentation/#ancestors-in-mro_2","text":"text_explainability.data.augmentation.LocalTokenPertubator instancelib.pertubations.base.MultiplePertubator instancelib.pertubations.base.ChildGenerator abc.ABC typing.Generic text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/augmentation/#descendants_1","text":"text_explainability.data.augmentation.LeaveOut","title":"Descendants"},{"location":"reference/text_explainability/data/augmentation/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/text_explainability/data/augmentation/#binary_inactive_2","text":"1 2 3 4 def binary_inactive ( inactive , length ) -> numpy . ndarray View Source 1 2 3 4 5 6 7 8 9 10 11 @staticmethod def binary_inactive(inactive, length) -> np.ndarray: res = np.ones(length, dtype=int) inactive = [res for res in inactive] res[inactive] = 0 return res","title":"binary_inactive"},{"location":"reference/text_explainability/data/augmentation/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/augmentation/#discard_children_2","text":"1 2 3 4 def discard_children ( self , parent : instancelib . instances . text . TextInstance ) -> None Discard the generated children for a given parent. View Source 1 2 3 4 5 def discard_children(self, parent: TextInstance) -> None: \"\"\"Discard the generated children for a given parent.\"\"\" self.env.discard_children(parent)","title":"discard_children"},{"location":"reference/text_explainability/data/augmentation/#get_children_2","text":"1 2 3 4 def get_children ( self , parent : instancelib . instances . text . TextInstance ) -> instancelib . instances . base . InstanceProvider Get the children of a given parent. View Source 1 2 3 4 5 def get_children(self, parent: TextInstance) -> InstanceProvider[TextInstance, Any, Any, Any, Any]: \"\"\"Get the children of a given parent.\"\"\" return self.env.get_children(parent)","title":"get_children"},{"location":"reference/text_explainability/data/augmentation/#perturb_2","text":"1 2 3 4 5 6 7 8 9 10 def perturb ( self , tokenized_instance : Iterable [ str ], n_samples : int = 50 , sequential : bool = True , contiguous : bool = False , min_changes : int = 1 , max_changes : int = 10000 , add_background_instance : bool = False ) -> Iterator [ Tuple [ Iterable [ str ], Iterable [ int ]]] Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Parameters: Name Type Description Default tokenized_instance Iterable[str] Tokenized instance to apply perturbations to. None n_samples int Number of samples to return. Defaults to 50. 50 sequential bool Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. None contiguous bool Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. False min_changes int Minimum number of tokens changes (1+). Defaults to 1. 1 max_changes int Maximum number of tokens changed. Defaults to 10000. 10000 add_background_instance bool Add an additional instance with all tokens replaced. Defaults to False. None Yields: Type Description Iterator[Sequence[Iterable[str], Iterable[int]]] Pertubed text instances and indices where perturbation were applied. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def perturb(self, tokenized_instance: Iterable[str], n_samples: int = 50, sequential: bool = True, contiguous: bool = False, min_changes: int = 1, max_changes: int = 10000, add_background_instance: bool = False) -> Iterator[Tuple[Iterable[str], Iterable[int]]]: \"\"\"Perturb a tokenized instance by replacing it with a single replacement token (e.g. 'UNKWRDZ'), which is assumed not to be part of the original tokens. Args: tokenized_instance (Iterable[str]): Tokenized instance to apply perturbations to. n_samples (int, optional): Number of samples to return. Defaults to 50. sequential (bool, optional): Whether to sample sequentially based on length (first length one, then two, etc.). Defaults to True. contiguous (bool, optional): Whether to remove contiguous sequences of tokens (n-grams). Defaults to False. min_changes (int, optional): Minimum number of tokens changes (1+). Defaults to 1. max_changes (int, optional): Maximum number of tokens changed. Defaults to 10000. add_background_instance (bool, optional): Add an additional instance with all tokens replaced. Defaults to False. Yields: Iterator[Sequence[Iterable[str], Iterable[int]]]: Pertubed text instances and indices where perturbation were applied. \"\"\" instance_len = sum(1 for _ in tokenized_instance) min_changes = min(max(min_changes, 1), instance_len) max_changes = min(instance_len, max_changes) assert min_changes <= max_changes, \\ f'Unable to produce any perturbations since min_changes={min_changes} and max_changes={max_changes}' rand = np.random.RandomState(self._seed) def get_inactive(inactive_range): inactive = TokenReplacement.binary_inactive(inactive_range, instance_len) return self._replace(tokenized_instance, inactive), inactive if sequential: if contiguous: # n-grams of length size, up to n_samples for size in range(min_changes, max_changes + 1): n_contiguous = instance_len - size if n_contiguous <= n_samples: n_samples -= n_contiguous for start in range(instance_len - size + 1): yield get_inactive(range(start, start + size)) else: for start in rand.choice(instance_len - size + 1, replace=False): yield get_inactive(range(start, start + size)) break else: # used by SHAP for size in range(min_changes, max_changes + 1): n_choose_k = math.comb(instance_len, size) if n_choose_k <= n_samples: # make all combinations of length size n_samples -= n_choose_k for disable in itertools.combinations(range(instance_len), size): yield get_inactive(disable) else: # fill up remainder with random samples of length size for _ in range(n_samples): yield get_inactive(rand.choice(instance_len, size, replace=False)) break else: sample = rand.randint(min_changes, max_changes + 1, n_samples) for size in sample: if contiguous: # use n-grams start = rand.choice(max_changes - size + 1, replace=False) inactive = TokenReplacement.binary_inactive(range(start, start + size), instance_len) # used by LIME, # https://github.com/marcotcr/lime/blob/a2c7a6fb70bce2e089cb146a31f483bf218875eb/lime/lime_text.py#L436 else: inactive = TokenReplacement.binary_inactive(rand.choice(instance_len, size, replace=False), instance_len) yield self._replace(tokenized_instance, inactive), inactive if add_background_instance: inactive = np.zeros(instance_len) yield self._replace(tokenized_instance, inactive), inactive","title":"perturb"},{"location":"reference/text_explainability/data/augmentation/#register_child_2","text":"1 2 3 4 5 def register_child ( self , parent : ~ IT , child : ~ IT ) -> None View Source 1 2 3 def register_child(self, parent: IT, child: IT) -> None: self.env.all_datapoints.add_child(parent, child)","title":"register_child"},{"location":"reference/text_explainability/data/embedding/","text":"Module text_explainability.data.embedding Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 \"\"\"Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods \"\"\" from typing import Union, Callable import numpy as np from instancelib.instances.memory import MemoryBucketProvider from text_explainability.default import Readable def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA, NMF from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' assert method in methods.keys(), f'Unknown method \"{method}\". Choose from {list(methods.keys())}.' if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Functions as_2d 1 2 3 4 5 def as_2d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 2 dimensions. View Source 1 2 3 4 5 def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) as_3d 1 2 3 4 5 def as_3d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 3 dimensions. View Source 1 2 3 4 5 def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) as_n_dimensional 1 2 3 4 5 6 def as_n_dimensional ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], n : int = 2 , method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors into n dimensions. Parameters: Name Type Description Default vectors Union[np.ndarray, list, MemoryBucketProvider] Vectors or BucketProvider with vectorized instances. None n int Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. 2 method str Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. None **kwargs None Optional arguments passed to method constructor. None Returns: Type Description np.ndarray Vectors summarized in n dimensions. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA, NMF from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' assert method in methods.keys(), f'Unknown method \"{method}\". Choose from {list(methods.keys())}.' if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) Classes CountVectorizer 1 2 3 class CountVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Ancestors (in MRO) text_explainability.data.embedding.Embedder text_explainability.default.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings Embedder 1 2 3 class Embedder ( model_fn : Callable ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) Ancestors (in MRO) text_explainability.default.Readable Descendants text_explainability.data.embedding.SentenceTransformer text_explainability.data.embedding.CountVectorizer text_explainability.data.embedding.TfidfVectorizer Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings SentenceTransformer 1 2 3 4 class SentenceTransformer ( model_name : str = 'distiluse-base-multilingual-cased-v1' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) Ancestors (in MRO) text_explainability.data.embedding.Embedder text_explainability.default.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings TfidfVectorizer 1 2 3 class TfidfVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) Ancestors (in MRO) text_explainability.data.embedding.Embedder text_explainability.default.Readable Methods embed 1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"Embedding"},{"location":"reference/text_explainability/data/embedding/#module-text_explainabilitydataembedding","text":"Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 \"\"\"Embed text instances into numerical vectors. Todo: * Add more sentence embedding methods \"\"\" from typing import Union, Callable import numpy as np from instancelib.instances.memory import MemoryBucketProvider from text_explainability.default import Readable def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA, NMF from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' assert method in methods.keys(), f'Unknown method \"{method}\". Choose from {list(methods.keys())}.' if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors) def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs) class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances) class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs)) class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray()) class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"Module text_explainability.data.embedding"},{"location":"reference/text_explainability/data/embedding/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/data/embedding/#as_2d","text":"1 2 3 4 5 def as_2d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 2 dimensions. View Source 1 2 3 4 5 def as_2d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 2 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs)","title":"as_2d"},{"location":"reference/text_explainability/data/embedding/#as_3d","text":"1 2 3 4 5 def as_3d ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors in 3 dimensions. View Source 1 2 3 4 5 def as_3d(vectors: Union[np.ndarray, list, MemoryBucketProvider], method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors in 3 dimensions.\"\"\" return as_n_dimensional(vectors=vectors, n=2, method=method, **kwargs)","title":"as_3d"},{"location":"reference/text_explainability/data/embedding/#as_n_dimensional","text":"1 2 3 4 5 6 def as_n_dimensional ( vectors : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ], n : int = 2 , method : str = 'pca' , ** kwargs ) -> numpy . ndarray Summarize vectors into n dimensions. Parameters: Name Type Description Default vectors Union[np.ndarray, list, MemoryBucketProvider] Vectors or BucketProvider with vectorized instances. None n int Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. 2 method str Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. None **kwargs None Optional arguments passed to method constructor. None Returns: Type Description np.ndarray Vectors summarized in n dimensions. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def as_n_dimensional(vectors: Union[np.ndarray, list, MemoryBucketProvider], n: int = 2, method: str = 'pca', **kwargs) -> np.ndarray: \"\"\"Summarize vectors into n dimensions. Args: vectors (Union[np.ndarray, list, MemoryBucketProvider]): Vectors or BucketProvider with vectorized instances. n (int, optional): Number of dimensions (should be low, e.g. 2 or 3). Defaults to 2. method (str, optional): Method used for dimensionality reduction. Choose from ['pca', 'kernel_pca', 'incremental_pca', 'nmf', 'tsne']. Defaults to 'pca'. **kwargs: Optional arguments passed to method constructor. Returns: np.ndarray: Vectors summarized in n dimensions. \"\"\" from sklearn.decomposition import PCA, KernelPCA, IncrementalPCA, NMF from sklearn.manifold import TSNE methods = {'pca': PCA, 'kernel_pca': KernelPCA, 'incremental_pca': IncrementalPCA, 'nmf': NMF, 'tsne': TSNE} # Default to `init='pca'` for tsne to ensure stability if method == 'tsne' and 'init' not in kwargs: kwargs['init'] = 'pca' assert method in methods.keys(), f'Unknown method \"{method}\". Choose from {list(methods.keys())}.' if isinstance(vectors, MemoryBucketProvider): vectors = vectors.bulk_get_vectors(list(vectors))[-1] return methods[method](n_components=n, **kwargs).fit_transform(vectors)","title":"as_n_dimensional"},{"location":"reference/text_explainability/data/embedding/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/embedding/#countvectorizer","text":"1 2 3 class CountVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class CountVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.CountVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.CountVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \"\"\" from sklearn.feature_extraction.text import CountVectorizer as Count self.model = Count(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"CountVectorizer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro","text":"text_explainability.data.embedding.Embedder text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#embedder","text":"1 2 3 class Embedder ( model_fn : Callable ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class Embedder(Readable): def __init__(self, model_fn: Callable): \"\"\"Embedding model base class to transform instances into vectors. Args: model_fn (Callable): Model that embeds instances (transforms into vectors). \"\"\" self.model_fn = model_fn def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings def __call__(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Calls the `self.embed()` function.\"\"\" return self.embed(instances)","title":"Embedder"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_1","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#descendants","text":"text_explainability.data.embedding.SentenceTransformer text_explainability.data.embedding.CountVectorizer text_explainability.data.embedding.TfidfVectorizer","title":"Descendants"},{"location":"reference/text_explainability/data/embedding/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_1","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#sentencetransformer","text":"1 2 3 4 class SentenceTransformer ( model_name : str = 'distiluse-base-multilingual-cased-v1' , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class SentenceTransformer(Embedder): def __init__(self, model_name: str = 'distiluse-base-multilingual-cased-v1', **kwargs): \"\"\"Embed sentences using the `Sentence Transformers`_ package. By default requires and active internet connection, or provide the name of a local `model_name`. Args: model_name (str, optional): Name of Sentence Transformer model. See https://www.sbert.net/docs/pretrained_models.html for model names. Defaults to 'distiluse-base-multilingual-cased-v1'. **kwargs: Optional arguments to be passed to `SentenceTransformer.encode()` function. See https://www.sbert.net/examples/applications/computing-embeddings/README.html .. _Sentence Transformers: https://github.com/UKPLab/sentence-transformers \"\"\" from sentence_transformers import SentenceTransformer as SentTransformer self.model = SentTransformer(model_name) super().__init__(lambda x: self.model.encode(x, **kwargs))","title":"SentenceTransformer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_2","text":"text_explainability.data.embedding.Embedder text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_2","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/embedding/#tfidfvectorizer","text":"1 2 3 class TfidfVectorizer ( ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TfidfVectorizer(Embedder): def __init__(self, **kwargs): \"\"\"Embed sentences using `sklearn.TfidfVectorizer`_. Args: **kwargs: Optional arguments passed for `sklearn.TfidfVectorizer()` construction. .. _sklearn.TfidfVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \"\"\" from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf self.model = Tfidf(**kwargs) super().__init__(lambda x: self.model.fit_transform(x).toarray())","title":"TfidfVectorizer"},{"location":"reference/text_explainability/data/embedding/#ancestors-in-mro_3","text":"text_explainability.data.embedding.Embedder text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/embedding/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/data/embedding/#embed_3","text":"1 2 3 4 def embed ( self , instances : Union [ numpy . ndarray , list , instancelib . instances . memory . MemoryBucketProvider ] ) -> Union [ numpy . ndarray , instancelib . instances . memory . MemoryBucketProvider ] Embed instances (transform into numerical vectors). Parameters: Name Type Description Default instances Union[np.ndarray, list, MemoryBucketProvider] Sequence of instances. None Returns: Type Description Union[np.ndarray, MemoryBucketProvider] Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def embed(self, instances: Union[np.ndarray, list, MemoryBucketProvider]) -> Union[np.ndarray, MemoryBucketProvider]: \"\"\"Embed instances (transform into numerical vectors). Args: instances (Union[np.ndarray, list, MemoryBucketProvider]): Sequence of instances. Returns: Union[np.ndarray, MemoryBucketProvider]: Embedded instances (provided back into the BucketProvider if it was originally passed as a BucketProvider). \"\"\" is_provider = isinstance(instances, MemoryBucketProvider) if is_provider: instances_ = instances instances = list(instances.all_data()) embeddings = self.model_fn(instances) if is_provider: for idx, embedding in zip(instances_, embeddings): instances_[idx].vector = embedding return instances_ if not isinstance(embeddings, np.ndarray): embeddings = np.array(embeddings) return embeddings","title":"embed"},{"location":"reference/text_explainability/data/sampling/","text":"Module text_explainability.data.sampling Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 \"\"\"Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) \"\"\" from typing import Dict, Sequence, Callable, Optional, Union import numpy as np from instancelib.instances.memory import MemoryBucketProvider from instancelib.instances.text import MemoryTextInstance from instancelib.labels.memory import MemoryLabelProvider from instancelib.labels.base import LabelProvider from instancelib.machinelearning.base import AbstractClassifier from text_explainability.data.embedding import Embedder, TfidfVectorizer from text_explainability.data.weights import exponential_kernel from text_explainability.default import Readable class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[MemoryTextInstance]: \"\"\"Select instances from provider by keys.\"\"\" return [self.instances[i] for i in keys] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) class KMedoids(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()} Classes KMedoids 1 2 3 4 5 class KMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class KMedoids(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) Ancestors (in MRO) text_explainability.data.sampling.PrototypeSampler text_explainability.default.Readable Instance variables 1 embedded Methods prototypes 1 2 3 4 5 6 def prototypes ( self , n : int = 5 , metric : Union [ str , Callable ] = 'cosine' , ** kwargs ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes (most representative samples) using k-Medoids _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 metrics Union[str, Callable] Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See pairwise distances for a full list. Defaults to 'cosine'. None **kwargs None Optional arguments passed to k-Medoids _ constructor. None Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) LabelwiseKMedoids 1 2 3 4 5 6 class LabelwiseKMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) Ancestors (in MRO) text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.default.Readable Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} LabelwiseMMDCritic 1 2 3 4 5 6 class LabelwiseMMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16ddf9a60 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()} Ancestors (in MRO) text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.default.Readable Methods criticisms 1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n criticisms (instances not well represented by prototypes). Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of criticisms. Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} LabelwisePrototypeSampler 1 2 3 4 5 6 7 class LabelwisePrototypeSampler ( sampler : text_explainability . data . sampling . PrototypeSampler , instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider , instancelib . machinelearning . base . AbstractClassifier ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} Ancestors (in MRO) text_explainability.default.Readable Descendants text_explainability.data.sampling.LabelwiseKMedoids text_explainability.data.sampling.LabelwiseMMDCritic Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} MMDCritic 1 2 3 4 5 class MMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16ddf9a60 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} Ancestors (in MRO) text_explainability.data.sampling.PrototypeSampler text_explainability.default.Readable Instance variables 1 embedded Methods criticisms 1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n criticisms (instances not well represented by prototypes), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Sequence[MemoryTextInstance] List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes (most representatitve instances), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes PrototypeSampler 1 2 3 4 class PrototypeSampler ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '> ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[MemoryTextInstance]: \"\"\"Select instances from provider by keys.\"\"\" return [self.instances[i] for i in keys] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) Ancestors (in MRO) text_explainability.default.Readable Descendants text_explainability.data.sampling.KMedoids text_explainability.data.sampling.MMDCritic Instance variables 1 embedded Methods prototypes 1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses')","title":"Sampling"},{"location":"reference/text_explainability/data/sampling/#module-text_explainabilitydatasampling","text":"Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 \"\"\"Sample an (informative) subset from the data. Todo: * Sample (informative?) subset from data * Refactor to make sampling base class * Add ability to perform MMD critic on a subset (e.g. single class) \"\"\" from typing import Dict, Sequence, Callable, Optional, Union import numpy as np from instancelib.instances.memory import MemoryBucketProvider from instancelib.instances.text import MemoryTextInstance from instancelib.labels.memory import MemoryLabelProvider from instancelib.labels.base import LabelProvider from instancelib.machinelearning.base import AbstractClassifier from text_explainability.data.embedding import Embedder, TfidfVectorizer from text_explainability.data.weights import exponential_kernel from text_explainability.default import Readable class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[MemoryTextInstance]: \"\"\"Select instances from provider by keys.\"\"\" return [self.instances[i] for i in keys] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs) class KMedoids(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_) class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)} class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()} class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed) class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"Module text_explainability.data.sampling"},{"location":"reference/text_explainability/data/sampling/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/data/sampling/#kmedoids","text":"1 2 3 4 5 class KMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class KMedoids(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Sampling prototypes (representative samples) based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(instances, embedder) self._seed = seed def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_)","title":"KMedoids"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro","text":"text_explainability.data.sampling.PrototypeSampler text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#instance-variables","text":"1 embedded","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes","text":"1 2 3 4 5 6 def prototypes ( self , n : int = 5 , metric : Union [ str , Callable ] = 'cosine' , ** kwargs ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes (most representative samples) using k-Medoids _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 metrics Union[str, Callable] Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See pairwise distances for a full list. Defaults to 'cosine'. None **kwargs None Optional arguments passed to k-Medoids _ constructor. None Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prototypes(self, n: int = 5, metric: Union[str, Callable] = 'cosine', **kwargs) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representative samples) using `k-Medoids`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. metrics (Union[str, Callable], optional): Distance metric used to calculate medoids (e.g. 'cosine', 'euclidean' or your own function). See `pairwise distances` for a full list. Defaults to 'cosine'. **kwargs: Optional arguments passed to `k-Medoids`_ constructor. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html .. _pairwise distances: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html \"\"\" from sklearn_extra.cluster import KMedoids kmedoids = KMedoids(n_clusters=n, metric=metric, random_state=self._seed, **kwargs).fit(self.embedded) return self._select_from_provider(kmedoids.medoid_indices_)","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#labelwisekmedoids","text":"1 2 3 4 5 6 class LabelwiseKMedoids ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, seed : int = 0 ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class LabelwiseKMedoids(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, seed: int = 0): \"\"\"Select prototypes for each label based on embedding distances using `k-Medoids`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. seed (int, optional): Seed for reproducibility. Defaults to 0. .. _k-Medoids: https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html \"\"\" super().__init__(KMedoids, instances=instances, labels=labels, embedder=embedder, seed=seed)","title":"LabelwiseKMedoids"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_1","text":"text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_1","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#labelwisemmdcritic","text":"1 2 3 4 5 6 class LabelwiseMMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16ddf9a60 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class LabelwiseMMDCritic(LabelwisePrototypeSampler): def __init__(self, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider], embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms for each label based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(MMDCritic, instances=instances, labels=labels, embedder=embedder, kernel=kernel) def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()} def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes and criticisms for each label. Args: n_prototypes (int, optional): Number of prototypes to select. Defaults to 5. n_criticisms (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes and criticisms. \"\"\" return {label: sampler(n_prototypes=n_prototypes, n_criticisms=n_criticisms, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"LabelwiseMMDCritic"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_2","text":"text_explainability.data.sampling.LabelwisePrototypeSampler text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#criticisms","text":"1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n criticisms (instances not well represented by prototypes). Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of criticisms. Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` criticisms (instances not well represented by prototypes). Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of criticisms. \"\"\" return {label: sampler.criticisms(n=n, regularizer=regularizer) for label, sampler in self._samplers.items()}","title":"criticisms"},{"location":"reference/text_explainability/data/sampling/#prototypes_2","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#labelwiseprototypesampler","text":"1 2 3 4 5 6 7 class LabelwisePrototypeSampler ( sampler : text_explainability . data . sampling . PrototypeSampler , instances : instancelib . instances . memory . MemoryBucketProvider , labels : Union [ Sequence [ str ], Sequence [ int ], instancelib . labels . base . LabelProvider , instancelib . machinelearning . base . AbstractClassifier ], embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class LabelwisePrototypeSampler(Readable): def __init__(self, sampler: PrototypeSampler, instances: MemoryBucketProvider, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier], embedder: Embedder = TfidfVectorizer, **kwargs): \"\"\"Apply `PrototypeSampler()` for each label. Args: sampler (PrototypeSampler): Prototype sampler to construct (e.g. `KMedoids`, `MMDCritic`) instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). labels (Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): Ground-truth or predicted labels, providing the groups (e.g. classes) in which to subdivide the instances. embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to `_setup_instances()` constructor. \"\"\" self.sampler = sampler if isinstance(sampler, type) else self.sampler.__class__ self.instances = instances self._get_labels(labels) self._setup_samplers(embedder, **kwargs) def _get_labels(self, labels: Union[Sequence[str], Sequence[int], LabelProvider, AbstractClassifier]): \"\"\"Transform the labels into a `LabelProvider`.\"\"\" if not isinstance(labels, LabelProvider): if isinstance(labels, AbstractClassifier): labels_ = labels.predict(self.instances) else: labels_ = [(id, frozenset({label})) for id, label in zip(list(self.instances), labels)] labels = MemoryLabelProvider.from_tuples(labels_) self.labels = labels def _setup_samplers(self, embedder: Embedder, **kwargs): \"\"\"Setup a sampler for each label in `self.labels.labelset`. Args: embedder (Embedder): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. **kwargs: Additional arguments passed to sampler constructor. \"\"\" import copy def select_by_label(label): instances = copy.deepcopy(self.instances) keys_to_keep = self.labels.get_instances_by_label(label) instances._remove_from_bucket(frozenset(list(instances)).difference(keys_to_keep)) return instances self._samplers = {label: self.sampler(instances=select_by_label(label), embedder=embedder, **kwargs) for label in self.labels.labelset} self.samplers = self._samplers def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()} def __call__(self, n: int = 5) -> Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: \"\"\"Generate prototypes for each label. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Dict[str, Sequence[MemoryTextInstance]]]: Dictionary with labels and corresponding dictionary containing prototypes. \"\"\" return {label: {'prototypes': sampler.prototypes(n=n)} for label, sampler in self._samplers.items()}","title":"LabelwisePrototypeSampler"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_3","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#descendants","text":"text_explainability.data.sampling.LabelwiseKMedoids text_explainability.data.sampling.LabelwiseMMDCritic","title":"Descendants"},{"location":"reference/text_explainability/data/sampling/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_3","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Dict [ str , Sequence [ instancelib . instances . text . MemoryTextInstance ]] Select n prototypes (most representatitve instances). Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Dict[str, Sequence[MemoryTextInstance]] Dictionary with labels and corresponding list of prototypes. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def prototypes(self, n: int = 5) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Select `n` prototypes (most representatitve instances). Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary with labels and corresponding list of prototypes. \"\"\" return {label: sampler.prototypes(n=n) for label, sampler in self._samplers.items()}","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#mmdcritic","text":"1 2 3 4 5 class MMDCritic ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '>, kernel : Callable = < function exponential_kernel at 0x16ddf9a60 > ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class MMDCritic(PrototypeSampler): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer, kernel: Callable = exponential_kernel): \"\"\"Select prototypes and criticisms based on embedding distances using `MMD-Critic`_. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. kernel (Callable, optional): Kernel to calculate distances. Defaults to exponential_kernel. .. _MMD-critic: https://christophm.github.io/interpretable-ml-book/proto.html \"\"\" super().__init__(instances, embedder) self.kernel = kernel self._calculate_kernel() self._prototypes = None self._criticisms = None def _calculate_kernel(self): \"\"\"Calculate kernel `K` and column totals `colsum`.\"\"\" self.K = self.kernel(self.embedded, 1.0 / self.embedded.shape[1]) self.colsum = np.sum(self.K, axis=0) / self.embedded.shape[1] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms def __call__(self, n_prototypes: int = 5, n_criticisms: int = 5, regularizer: Optional[str] = None) -> Dict[str, Sequence[MemoryTextInstance]]: \"\"\"Calculate prototypes and criticisms for the provided instances. Args: n_prototypes (int, optional): Number of prototypes. Defaults to 5. n_criticisms (int, optional): Number of criticisms. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Returns: Dict[str, Sequence[MemoryTextInstance]]: Dictionary containing prototypes and criticisms. \"\"\" return {'prototypes': self.prototypes(n=n_prototypes), 'criticisms': self.criticisms(n=n_criticisms, regularizer=regularizer)}","title":"MMDCritic"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_4","text":"text_explainability.data.sampling.PrototypeSampler text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#instance-variables_1","text":"1 embedded","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods_4","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#criticisms_1","text":"1 2 3 4 5 def criticisms ( self , n : int = 5 , regularizer : Optional [ str ] = None ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n criticisms (instances not well represented by prototypes), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of criticisms to select. Defaults to 5. 5 regularizer Optional[str] Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. None Returns: Type Description Sequence[MemoryTextInstance] List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py Raises: Type Description Exception MMDCritic.prototypes() must first be run before being able to determine the criticisms. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def criticisms(self, n: int = 5, regularizer: Optional[str] = None) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` criticisms (instances not well represented by prototypes), using `MMD-critic implementation`_. Args: n (int, optional): Number of criticisms to select. Defaults to 5. regularizer (Optional[str], optional): Regularization method. Choose from [None, 'logdet', 'iterative']. Defaults to None. Raises: Exception: `MMDCritic.prototypes()` must first be run before being able to determine the criticisms. Returns: Sequence[MemoryTextInstance]: List of criticism instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" if self._prototypes is None: raise Exception('Calculating criticisms requires prototypes. Run `MMDCritic.prototypes()` first.') regularizers = {None, 'logdet', 'iterative'} assert regularizer in regularizers, \\ f'Unknown regularizer \"{regularizer}\", choose from {regularizers}.' assert n <= (len(self.instances) - len(self._prototypes)), \\ f'Cannot select more than instances excluding prototypes ({len(self.instances) - len(self._prototypes)})' prototypes = np.array([p.identifier for p in self._prototypes]) K = self.K colsum = self.colsum sample_indices = np.arange(0, len(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] is_selected[prototypes] = n + 1 inverse_of_prev_selected = None for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] temp = K[prototypes, :][:, candidate_indices] s2 = np.sum(temp, axis=0) s2 /= prototypes.shape[0] s1 -= s2 s1 = np.abs(s1) if regularizer == 'logdet': diag = np.diagonal(K)[candidate_indices] if inverse_of_prev_selected is not None: temp = K[selected, :][:, candidate_indices] temp2 = np.dot(inverse_of_prev_selected, temp) reg = temp2 * temp regcolsum = np.sum(reg, axis=0) with np.errstate(divide='ignore'): reg = np.log(np.abs(diag - regcolsum)) s1 += reg else: with np.errstate(divide='ignore'): s1 -= np.log(np.abs(diag)) best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[(is_selected > 0) & (is_selected != (n + 1))] if regularizer == 'iterative': prototypes = np.concatenate([prototypes, np.expand_dims(best_sample_index, 0)]) if regularizer == 'logdet': inverse_of_prev_selected = np.linalg.pinv(K[selected, :][:, selected]) selected_in_order = selected[is_selected[(is_selected > 0) & (is_selected != (n + 1))].argsort()] self._criticisms = self._select_from_provider(selected_in_order) return self._criticisms","title":"criticisms"},{"location":"reference/text_explainability/data/sampling/#prototypes_4","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes (most representatitve instances), using MMD-critic implementation _. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes (most representatitve instances), using `MMD-critic implementation`_. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. .. _MMD-critic implementation: https://github.com/maxidl/MMD-critic/blob/main/mmd_critic.py \"\"\" assert n <= len(self.instances), f'Cannot select more than all instances ({len(self.instances)}.' K = self.K colsum = self.colsum.copy() * 2 sample_indices = np.array(list(self.instances)) is_selected = np.zeros_like(sample_indices) selected = sample_indices[is_selected > 0] for i in range(n): candidate_indices = sample_indices[is_selected == 0] s1 = colsum[candidate_indices] diag = np.diagonal(K)[candidate_indices] if selected.shape[0] == 0: s1 -= np.abs(diag) else: temp = K[selected, :][:, candidate_indices] s2 = np.sum(temp, axis=0) * 2 + diag s2 /= (selected.shape[0] + 1) s1 -= s2 best_sample_index = candidate_indices[np.argmax(s1)] is_selected[best_sample_index] = i + 1 selected = sample_indices[is_selected > 0] selected_in_order = selected[is_selected[is_selected > 0].argsort()] self._prototypes = self._select_from_provider(selected_in_order) return self._prototypes","title":"prototypes"},{"location":"reference/text_explainability/data/sampling/#prototypesampler","text":"1 2 3 4 class PrototypeSampler ( instances : instancelib . instances . memory . MemoryBucketProvider , embedder : text_explainability . data . embedding . Embedder = < class ' text_explainability . data . embedding . TfidfVectorizer '> ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PrototypeSampler(Readable): def __init__(self, instances: MemoryBucketProvider, embedder: Embedder = TfidfVectorizer): \"\"\"Generic class for sampling prototypes (representative samples) based on embedding distances. Args: instances (MemoryBucketProvider): Instances to select from (e.g. training set, all instance from class 0). embedder (Embedder, optional): Method to embed instances (if the `.vector` property is not yet set). Defaults to TfidfVectorizer. \"\"\" self.embedder = embedder() if isinstance(embedder, type) else embedder self.instances = self.embedder(instances) if any(instances[i].vector is None for i in instances) \\ else instances @property def embedded(self) -> np.ndarray: return np.stack(self.instances.bulk_get_vectors(list(self.instances))[-1]) def _select_from_provider(self, keys: Sequence[int]) -> Sequence[MemoryTextInstance]: \"\"\"Select instances from provider by keys.\"\"\" return [self.instances[i] for i in keys] def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses') def __call__(self, *args, **kwargs): return self.prototypes(*args, **kwargs)","title":"PrototypeSampler"},{"location":"reference/text_explainability/data/sampling/#ancestors-in-mro_5","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/data/sampling/#descendants_1","text":"text_explainability.data.sampling.KMedoids text_explainability.data.sampling.MMDCritic","title":"Descendants"},{"location":"reference/text_explainability/data/sampling/#instance-variables_2","text":"1 embedded","title":"Instance variables"},{"location":"reference/text_explainability/data/sampling/#methods_5","text":"","title":"Methods"},{"location":"reference/text_explainability/data/sampling/#prototypes_5","text":"1 2 3 4 def prototypes ( self , n : int = 5 ) -> Sequence [ instancelib . instances . text . MemoryTextInstance ] Select n prototypes. Parameters: Name Type Description Default n int Number of prototypes to select. Defaults to 5. 5 Returns: Type Description Sequence[MemoryTextInstance] List of prototype instances. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def prototypes(self, n: int = 5) -> Sequence[MemoryTextInstance]: \"\"\"Select `n` prototypes. Args: n (int, optional): Number of prototypes to select. Defaults to 5. Returns: Sequence[MemoryTextInstance]: List of prototype instances. \"\"\" raise NotImplementedError('Implemented in subclasses')","title":"prototypes"},{"location":"reference/text_explainability/data/weights/","text":"Module text_explainability.data.weights None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn.metrics.pairwise import pairwise_distances as pwd def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2)) Functions exponential_kernel 1 2 3 4 def exponential_kernel ( d , kw ) View Source 1 2 3 def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2)) pairwise_distances 1 2 3 4 5 6 def pairwise_distances ( a , b , metric = 'cosine' , multiply = 100 ) View Source 1 2 3 4 5 6 7 def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply","title":"Weights"},{"location":"reference/text_explainability/data/weights/#module-text_explainabilitydataweights","text":"None None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn.metrics.pairwise import pairwise_distances as pwd def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2))","title":"Module text_explainability.data.weights"},{"location":"reference/text_explainability/data/weights/#functions","text":"","title":"Functions"},{"location":"reference/text_explainability/data/weights/#exponential_kernel","text":"1 2 3 4 def exponential_kernel ( d , kw ) View Source 1 2 3 def exponential_kernel(d, kw): return np.sqrt(np.exp(-(d ** 2) / kw ** 2))","title":"exponential_kernel"},{"location":"reference/text_explainability/data/weights/#pairwise_distances","text":"1 2 3 4 5 6 def pairwise_distances ( a , b , metric = 'cosine' , multiply = 100 ) View Source 1 2 3 4 5 6 7 def pairwise_distances(a, b, metric='cosine', multiply=100): if b.ndim == 1: b = b.reshape(1, -1) return pwd(a, b, metric=metric).ravel() * multiply","title":"pairwise_distances"},{"location":"reference/text_explainability/generation/","text":"Module text_explainability.generation Feature selection and local/global surrogate model generation. None View Source 1 \"\"\"Feature selection and local/global surrogate model generation.\"\"\" Sub-modules text_explainability.generation.feature_selection text_explainability.generation.return_types text_explainability.generation.surrogate text_explainability.generation.target_encoding","title":"Index"},{"location":"reference/text_explainability/generation/#module-text_explainabilitygeneration","text":"Feature selection and local/global surrogate model generation. None View Source 1 \"\"\"Feature selection and local/global surrogate model generation.\"\"\"","title":"Module text_explainability.generation"},{"location":"reference/text_explainability/generation/#sub-modules","text":"text_explainability.generation.feature_selection text_explainability.generation.return_types text_explainability.generation.surrogate text_explainability.generation.target_encoding","title":"Sub-modules"},{"location":"reference/text_explainability/generation/feature_selection/","text":"Module text_explainability.generation.feature_selection Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 \"\"\"Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern \"\"\" import numpy as np from typing import Optional from sklearn.linear_model import LassoLarsIC, Lasso, lars_path from text_explainability.generation.surrogate import LinearSurrogate from text_explainability.default import Readable class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'forward_selection requires a local linear model' n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.array(used_features) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'highest_weights requires a local linear model' self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.array([x[0] for x in feature_weights[:n_features]]) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.array(used_features) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" assert criterion in ['aic', 'bic'], f'Unknown criterion \"{criterion}\"' return np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0] def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # To-do: multiclass support? y = y[:, 0] return lars_path(X, y, max_iter=n_features)[1] def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: str = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: AssertionError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None: assert method not in ['forward_selection', 'highest_weights'], \\ f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ' \\ 'highest_weights' assert method in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg'], \\ f'Unknown method \"{method}\"' n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) Classes FeatureSelector 1 2 3 class FeatureSelector ( model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'forward_selection requires a local linear model' n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.array(used_features) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'highest_weights requires a local linear model' self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.array([x[0] for x in feature_weights[:n_features]]) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.array(used_features) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" assert criterion in ['aic', 'bic'], f'Unknown criterion \"{criterion}\"' return np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0] def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # To-do: multiclass support? y = y[:, 0] return lars_path(X, y, max_iter=n_features)[1] def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: str = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: AssertionError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None: assert method not in ['forward_selection', 'highest_weights'], \\ f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ' \\ 'highest_weights' assert method in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg'], \\ f'Unknown method \"{method}\"' n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha) Ancestors (in MRO) text_explainability.default.Readable","title":"Feature Selection"},{"location":"reference/text_explainability/generation/feature_selection/#module-text_explainabilitygenerationfeature_selection","text":"Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 \"\"\"Feature selection methods for limiting explanation length. Todo: * Convert to factory design pattern \"\"\" import numpy as np from typing import Optional from sklearn.linear_model import LassoLarsIC, Lasso, lars_path from text_explainability.generation.surrogate import LinearSurrogate from text_explainability.default import Readable class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'forward_selection requires a local linear model' n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.array(used_features) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'highest_weights requires a local linear model' self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.array([x[0] for x in feature_weights[:n_features]]) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.array(used_features) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" assert criterion in ['aic', 'bic'], f'Unknown criterion \"{criterion}\"' return np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0] def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # To-do: multiclass support? y = y[:, 0] return lars_path(X, y, max_iter=n_features)[1] def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: str = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: AssertionError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None: assert method not in ['forward_selection', 'highest_weights'], \\ f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ' \\ 'highest_weights' assert method in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg'], \\ f'Unknown method \"{method}\"' n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha)","title":"Module text_explainability.generation.feature_selection"},{"location":"reference/text_explainability/generation/feature_selection/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/feature_selection/#featureselector","text":"1 2 3 class FeatureSelector ( model : Optional [ text_explainability . generation . surrogate . LinearSurrogate ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 class FeatureSelector(Readable): def __init__(self, model: Optional[LinearSurrogate] = None): \"\"\"[summary] Args: model (Optional[LinearSurrogate], optional): Linear surrogate used to calculate feature importance scores. Defaults to None. \"\"\" super().__init__() self.model = model if self.model is not None: self.model.alpha_zero() self.model.fit_intercept = True def _forward_selection(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with forward selection, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for y. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): [description]. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'forward_selection requires a local linear model' n_features = min(X.shape[1], n_features) used_features = [] for _ in range(n_features): max_ = -100000000 best = 0 for feature in range(X.shape[1]): if feature in used_features: continue self.model.fit(X[:, used_features + [feature]], y, weights=weights) score = self.model.score(X[:, used_features + [feature]], y, weights=weights) if score > max_: best = feature max_ = score used_features.append(best) return np.array(used_features) def _highest_weights(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection according to highest feature importance, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Raises: AssertionError: The local linear model used to calculate forward_selection was not defined. Returns: np.ndarray: Indices of selected features. .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" assert self.model is not None, 'highest_weights requires a local linear model' self.model.fit(X, y, weights=weights) weighted_data = self.model.feature_importances * X[0] feature_weights = sorted( zip(range(X.shape[1]), weighted_data), key=lambda x: np.abs(x[1]), reverse=True) return np.array([x[0] for x in feature_weights[:n_features]]) def _lasso_path(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10) -> np.ndarray: \"\"\"Feature selection with `LASSO`_, as used by `LIME`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. Returns: np.ndarray: Indices of selected features. .. _LASSO: https://en.wikipedia.org/wiki/Lasso_(statistics) .. _LIME: https://github.com/marcotcr/lime/blob/master/lime/lime_base.py \"\"\" if weights is None: weights = np.ones(X.shape[0]) weighted_data = ((X - np.average(X, axis=0, weights=weights)) * np.sqrt(weights[:, np.newaxis])) weighted_labels = ((y - np.average(y, weights=weights)) * np.sqrt(weights)) nonzero = range(weighted_data.shape[1]) _, _, coefs = lars_path(weighted_data, weighted_labels, method='lasso', verbose=False) for i in range(len(coefs.T) - 1, 0, -1): nonzero = coefs.T[i].nonzero()[0] if len(nonzero) <= n_features: break used_features = nonzero return np.array(used_features) def _information_criterion(self, X: np.ndarray, y: np.ndarray, criterion='aic') -> np.ndarray: \"\"\"AIC/BIC for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. criterion (str, optional): Whether to use `Akaike Information Criterion`_ (`aic`) or `Bayesian Information Criterion`_ (`bic`). Defaults to 'aic'. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap .. _Akaike Information Criterion: https://en.wikipedia.org/wiki/Akaike_information_criterion .. _Bayesian Information Criterion: https://en.wikipedia.org/wiki/Bayesian_information_criterion \"\"\" assert criterion in ['aic', 'bic'], f'Unknown criterion \"{criterion}\"' return np.nonzero(LassoLarsIC(criterion=criterion).fit(X, y).coef_)[0] def _l1_reg(self, X: np.ndarray, y: np.ndarray, n_features: int = 10, alpha: Optional[float] = None) -> np.ndarray: \"\"\"L1-regularization for feature selection, as used by `SHAP`_. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. n_features (int, optional): Number of features to select. Defaults to 10. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Returns: np.ndarray: Indices of selected features. .. _SHAP: https://github.com/slundberg/shap \"\"\" if alpha is not None: return np.nonzero(Lasso(alpha=alpha).fit(X, y).coef_)[0] # use n_features if y.ndim > 1: # To-do: multiclass support? y = y[:, 0] return lars_path(X, y, max_iter=n_features)[1] def __call__(self, X: np.ndarray, y: np.ndarray, weights: np.ndarray = None, n_features: int = 10, method: str = None, alpha: Optional[float] = None) -> np.ndarray: \"\"\"Apply feature selection for dataset X and targets y. Args: X (np.ndarray): Input data. y (np.ndarray): Prediction / ground-truth value for X. weights (np.ndarray, optional): Relative weights of X. Defaults to None. n_features (int, optional): Number of features to select. Defaults to 10. method (str, optional): Method to apply for feature selection, choose from `None`, `forward_selection`, `highest_weights`, `lasso_path`, `aic`, `bic`, `l1_reg`. Defaults to None. alpha (Optional[float], optional): Hyperparameter for L1 regularization. Defaults to None. Raises: AssertionError: Unknown method, or the requirements of a method have not been satisfied. Returns: np.ndarray: Indices of selected features. \"\"\" if self.model is None: assert method not in ['forward_selection', 'highest_weights'], \\ f'{self.__class__.__name__} requires a `model` to use methods forward_selection and ' \\ 'highest_weights' assert method in [None, 'forward_selection', 'highest_weights', 'lasso_path', 'aic', 'bic', 'l1_reg'], \\ f'Unknown method \"{method}\"' n_features = min(X.shape[1], n_features) # Do not perform feature selection, but return all if n_features == X.shape[1] and method not in ['aic', 'bic', 'l1_reg'] or method is None: return np.arange(X.shape[1]) # Perform feature selection if method == 'forward_selection': return self._forward_selection(X, y, weights=weights, n_features=n_features) elif method == 'highest_weights': return self._highest_weights(X, y, weights=weights, n_features=n_features) elif method == 'lasso_path': return self._lasso_path(X, y, weights=weights, n_features=n_features) elif method in ['aic', 'bic']: return self._information_criterion(X, y, criterion=method) elif method == 'l1_reg': return self._l1_reg(X, y, n_features=n_features, alpha=alpha)","title":"FeatureSelector"},{"location":"reference/text_explainability/generation/feature_selection/#ancestors-in-mro","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/","text":"Module text_explainability.generation.return_types General return types for global/local explanations. Todo: * add rule-based explanations * add named label support View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 \"\"\"General return types for global/local explanations. Todo: * add rule-based explanations * add named label support \"\"\" import numpy as np from typing import Union, Optional, Sequence, Dict, Tuple from instancelib import InstanceProvider from text_explainability.generation.surrogate import TreeSurrogate, RuleSurrogate class BaseReturnType: def __init__(self, used_features: Union[Sequence[str], Sequence[int]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Base return type. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" self._used_features = used_features self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' class FeatureList(BaseReturnType): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" super().__init__(used_features=used_features, labels=labels, labelset=labelset) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) def __str__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) class DataExplanation: def __init__(self, provider: InstanceProvider, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider self._original_instance = self._provider[next(iter(self._provider))] self._neighborhood_instances = self._provider.get_children(self._original_instance) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' class FeatureAttribution(ReadableDataMixin, FeatureList, DataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) class Rules(ReadableDataMixin, BaseReturnType, DataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Base return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) BaseReturnType.__init__(self, used_features=used_features, labels=labels, labelset=labelset) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules Classes BaseReturnType 1 2 3 4 5 class BaseReturnType ( used_features : Union [ Sequence [ str ], Sequence [ int ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class BaseReturnType: def __init__(self, used_features: Union[Sequence[str], Sequence[int]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Base return type. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" self._used_features = used_features self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' Descendants text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules Instance variables 1 labels Get labels property. 1 labelset Get label names property. 1 used_features Get used features property. Methods label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx DataExplanation 1 2 3 4 class DataExplanation ( provider : instancelib . instances . base . InstanceProvider , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DataExplanation: def __init__(self, provider: InstanceProvider, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider self._original_instance = self._provider[next(iter(self._provider))] self._neighborhood_instances = self._provider.get_children(self._original_instance) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances Descendants text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules Instance variables 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization. FeatureAttribution 1 2 3 4 5 6 7 8 9 10 class FeatureAttribution ( provider : instancelib . instances . base . InstanceProvider , scores : Sequence [ float ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , scores_stddev : Sequence [ float ] = None , base_score : float = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class FeatureAttribution(ReadableDataMixin, FeatureList, DataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) Ancestors (in MRO) text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.BaseReturnType text_explainability.generation.return_types.DataExplanation Instance variables 1 labels Get labels property. 1 labelset Get label names property. 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 scores Saved feature attribution scores. 1 used_features Names of features of the original instance. Methods get_raw_scores 1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) get_scores 1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx FeatureList 1 2 3 4 5 6 class FeatureList ( used_features : Union [ Sequence [ str ], Sequence [ int ]], scores : Union [ Sequence [ int ], Sequence [ float ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class FeatureList(BaseReturnType): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" super().__init__(used_features=used_features, labels=labels, labelset=labelset) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) def __str__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) Ancestors (in MRO) text_explainability.generation.return_types.BaseReturnType Descendants text_explainability.generation.return_types.FeatureAttribution Instance variables 1 labels Get labels property. 1 labelset Get label names property. 1 scores Saved scores (e.g. feature importance). 1 used_features Get used features property. Methods get_raw_scores 1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) get_scores 1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx ReadableDataMixin 1 2 3 4 5 class ReadableDataMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' Descendants text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules Instance variables 1 used_features Names of features of the original instance. Rules 1 2 3 4 5 6 7 8 class Rules ( provider : instancelib . instances . base . InstanceProvider , rules : Union [ Sequence [ str ], text_explainability . generation . surrogate . TreeSurrogate , text_explainability . generation . surrogate . RuleSurrogate ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class Rules(ReadableDataMixin, BaseReturnType, DataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Base return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) BaseReturnType.__init__(self, used_features=used_features, labels=labels, labelset=labelset) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules Ancestors (in MRO) text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.BaseReturnType text_explainability.generation.return_types.DataExplanation Instance variables 1 labels Get labels property. 1 labelset Get label names property. 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 rules 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 used_features Names of features of the original instance. Methods label_by_index 1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"Return Types"},{"location":"reference/text_explainability/generation/return_types/#module-text_explainabilitygenerationreturn_types","text":"General return types for global/local explanations. Todo: * add rule-based explanations * add named label support View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 \"\"\"General return types for global/local explanations. Todo: * add rule-based explanations * add named label support \"\"\" import numpy as np from typing import Union, Optional, Sequence, Dict, Tuple from instancelib import InstanceProvider from text_explainability.generation.surrogate import TreeSurrogate, RuleSurrogate class BaseReturnType: def __init__(self, used_features: Union[Sequence[str], Sequence[int]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Base return type. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" self._used_features = used_features self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})' class FeatureList(BaseReturnType): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" super().__init__(used_features=used_features, labels=labels, labelset=labelset) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) def __str__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()]) class DataExplanation: def __init__(self, provider: InstanceProvider, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider self._original_instance = self._provider[next(iter(self._provider))] self._neighborhood_instances = self._provider.get_children(self._original_instance) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})' class FeatureAttribution(ReadableDataMixin, FeatureList, DataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False) class Rules(ReadableDataMixin, BaseReturnType, DataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Base return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) BaseReturnType.__init__(self, used_features=used_features, labels=labels, labelset=labelset) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules","title":"Module text_explainability.generation.return_types"},{"location":"reference/text_explainability/generation/return_types/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/return_types/#basereturntype","text":"1 2 3 4 5 class BaseReturnType ( used_features : Union [ Sequence [ str ], Sequence [ int ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class BaseReturnType: def __init__(self, used_features: Union[Sequence[str], Sequence[int]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Base return type. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" self._used_features = used_features self._labels = labels self._labelset = labelset @property def labels(self): \"\"\"Get labels property.\"\"\" if self._labels is None: return self._labels return list(self._labels) @property def labelset(self): \"\"\"Get label names property.\"\"\" return self._labelset @property def used_features(self): \"\"\"Get used features property.\"\"\" return self._used_features def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx def __repr__(self) -> str: labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, used_features={self.used_features})'","title":"BaseReturnType"},{"location":"reference/text_explainability/generation/return_types/#descendants","text":"text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables","text":"1 labels Get labels property. 1 labelset Get label names property. 1 used_features Get used features property.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#label_by_index","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#dataexplanation","text":"1 2 3 4 class DataExplanation ( provider : instancelib . instances . base . InstanceProvider , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class DataExplanation: def __init__(self, provider: InstanceProvider, sampled: bool = False): \"\"\"Save the sampled/generated instances used to determine an explanation. Args: provider (InstanceProvider): Sampled or generated data, including original instance. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" self._provider = provider self._original_instance = self._provider[next(iter(self._provider))] self._neighborhood_instances = self._provider.get_children(self._original_instance) self.sampled = sampled @property def original_instance(self): \"\"\"The instance for which the feature attribution scores were calculated.\"\"\" return self._original_instance @property def perturbed_instances(self): \"\"\"Perturbed versions of the original instance, if `sampled=False` during initialization.\"\"\" return None if self.sampled else self._neighborhood_instances @property def sampled_instances(self): \"\"\"Sampled instances, if `sampled=True` during initialization.\"\"\" return self._neighborhood_instances if self.sampled else None @property def neighborhood_instances(self): \"\"\"Instances in the neighborhood (either sampled or perturbed).\"\"\" return self._neighborhood_instances","title":"DataExplanation"},{"location":"reference/text_explainability/generation/return_types/#descendants_1","text":"text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_1","text":"1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#featureattribution","text":"1 2 3 4 5 6 7 8 9 10 class FeatureAttribution ( provider : instancelib . instances . base . InstanceProvider , scores : Sequence [ float ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , scores_stddev : Sequence [ float ] = None , base_score : float = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class FeatureAttribution(ReadableDataMixin, FeatureList, DataExplanation): def __init__(self, provider: InstanceProvider, scores: Sequence[float], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, scores_stddev: Sequence[float] = None, base_score: float = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Create a `FeatureList` with additional information saved. The additional information contains the possibility to add standard deviations, base scores, and the sampled or generated instances used to calculate these scores. Args: provider (InstanceProvider): Sampled or generated data, including original instance. scores (Sequence[float]): Scores corresponding to the selected features. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Selected features for the explanation label. Defaults to None. scores_stddev (Sequence[float], optional): Standard deviation of each feature attribution score. Defaults to None. base_score (float, optional): Base score, to which all scores are relative. Defaults to None. labels (Optional[Sequence[int]], optional): Labels for outputs (e.g. classes). Defaults to None. labelset (Optional[Sequence[str]], optional): Label names corresponding to labels. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) FeatureList.__init__(self, used_features=used_features, scores=scores, labels=labels, labelset=labelset) self._base_score = base_score self._scores_stddev = scores_stddev @property def scores(self): \"\"\"Saved feature attribution scores.\"\"\" return self.get_scores(normalize=False)","title":"FeatureAttribution"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro","text":"text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.FeatureList text_explainability.generation.return_types.BaseReturnType text_explainability.generation.return_types.DataExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_2","text":"1 labels Get labels property. 1 labelset Get label names property. 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 scores Saved feature attribution scores. 1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#get_raw_scores","text":"1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores)","title":"get_raw_scores"},{"location":"reference/text_explainability/generation/return_types/#get_scores","text":"1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)}","title":"get_scores"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_1","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#featurelist","text":"1 2 3 4 5 6 class FeatureList ( used_features : Union [ Sequence [ str ], Sequence [ int ]], scores : Union [ Sequence [ int ], Sequence [ float ]], labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class FeatureList(BaseReturnType): def __init__(self, used_features: Union[Sequence[str], Sequence[int]], scores: Union[Sequence[int], Sequence[float]], labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None): \"\"\"Save scores per feature, grouped per label. Examples of scores are feature importance scores, or counts of features in a dataset. Args: used_features (Union[Sequence[str], Sequence[int]]): Used features per label. scores (Union[Sequence[int], Sequence[float]]): Scores per label. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. \"\"\" super().__init__(used_features=used_features, labels=labels, labelset=labelset) self._scores = scores def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores) def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)} @property def scores(self): \"\"\"Saved scores (e.g. feature importance).\"\"\" return self.get_scores(normalize=False) def __str__(self) -> str: return '\\n'.join([f'{a}: {str(b)}' for a, b in self.scores.items()])","title":"FeatureList"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_1","text":"text_explainability.generation.return_types.BaseReturnType","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#descendants_2","text":"text_explainability.generation.return_types.FeatureAttribution","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_3","text":"1 labels Get labels property. 1 labelset Get label names property. 1 scores Saved scores (e.g. feature importance). 1 used_features Get used features property.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#get_raw_scores_1","text":"1 2 3 4 def get_raw_scores ( self , normalize : bool = False ) -> numpy . ndarray Get saved scores per label as np.ndarray . Parameters: Name Type Description Default normalize bool Normalize scores (ensure they sum to one). Defaults to False. False Returns: Type Description np.ndarray Scores. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def get_raw_scores(self, normalize: bool = False) -> np.ndarray: \"\"\"Get saved scores per label as `np.ndarray`. Args: normalize (bool, optional): Normalize scores (ensure they sum to one). Defaults to False. Returns: np.ndarray: Scores. \"\"\" def feature_scores(scores): if not isinstance(scores, np.ndarray): scores = np.array(scores) if normalize: return scores / scores.sum(axis=0) return scores if isinstance(self._scores, dict): return {k: feature_scores(v) for k, v in self._scores.items()} return feature_scores(self._scores)","title":"get_raw_scores"},{"location":"reference/text_explainability/generation/return_types/#get_scores_1","text":"1 2 3 4 def get_scores ( self , normalize : bool = False ) -> Dict [ Union [ str , int ], Tuple [ Union [ str , int ], Union [ float , int ]]] Get scores per label. Parameters: Name Type Description Default normalize bool Whether to normalize the scores (sum to one). Defaults to False. False Returns: Type Description Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]] Scores per label, if no labelset is not set, defaults to 'all' View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def get_scores(self, normalize: bool = False) -> Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: \"\"\"Get scores per label. Args: normalize (bool, optional): Whether to normalize the scores (sum to one). Defaults to False. Returns: Dict[Union[str, int], Tuple[Union[str, int], Union[float, int]]]: Scores per label, if no `labelset` is not set, defaults to 'all' \"\"\" all_scores = self.get_raw_scores(normalize=normalize) if self.labels is None: return {'all': [(feature, score_) for feature, score_ in zip(self.used_features, all_scores)]} elif isinstance(self.used_features, dict): return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features[label], all_scores[i])] for i, label in enumerate(self.labels)} return {self.label_by_index(label): [(feature, score_) for feature, score_ in zip(self.used_features, all_scores[i])] for i, label in enumerate(self.labels)}","title":"get_scores"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_2","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/return_types/#readabledatamixin","text":"1 2 3 4 5 class ReadableDataMixin ( / , * args , ** kwargs ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ReadableDataMixin: @property def used_features(self): \"\"\"Names of features of the original instance.\"\"\" if hasattr(self.original_instance, 'tokenized'): return [self.original_instance.tokenized[i] for i in self._used_features] return list(self._used_features) def __repr__(self) -> str: sampled_or_perturbed = 'sampled' if self.sampled else 'perturbed' n = sum(1 for _ in self.neighborhood_instances) labels = [self.label_by_index(label) for label in self.labels] if self.labels is not None else None return f'{self.__class__.__name__}(labels={labels}, ' + \\ f'used_features={self.used_features}, n_{sampled_or_perturbed}_instances={n})'","title":"ReadableDataMixin"},{"location":"reference/text_explainability/generation/return_types/#descendants_3","text":"text_explainability.generation.return_types.FeatureAttribution text_explainability.generation.return_types.Rules","title":"Descendants"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_4","text":"1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#rules","text":"1 2 3 4 5 6 7 8 class Rules ( provider : instancelib . instances . base . InstanceProvider , rules : Union [ Sequence [ str ], text_explainability . generation . surrogate . TreeSurrogate , text_explainability . generation . surrogate . RuleSurrogate ], used_features : Union [ Sequence [ str ], Sequence [ int ], NoneType ] = None , labels : Optional [ Sequence [ int ]] = None , labelset : Optional [ Sequence [ str ]] = None , sampled : bool = False ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class Rules(ReadableDataMixin, BaseReturnType, DataExplanation): def __init__(self, provider: InstanceProvider, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate], used_features: Optional[Union[Sequence[str], Sequence[int]]] = None, labels: Optional[Sequence[int]] = None, labelset: Optional[Sequence[str]] = None, sampled: bool = False): \"\"\"Base return type. Args: provider (InstanceProvider): Sampled or generated data, including original instance. rules (Union[Sequence[str], TreeSurrogate, RuleSurrogate]): Rules applicable. used_features (Optional[Union[Sequence[str], Sequence[int]]]): Used features per label. Defaults to None. labels (Optional[Sequence[int]], optional): Label indices to include, if none provided defaults to 'all'. Defaults to None. labelset (Optional[Sequence[str]], optional): Lookup for label names. Defaults to None. sampled (bool, optional): Whether the data in the provider was sampled (True) or generated (False). Defaults to False. \"\"\" DataExplanation.__init__(self, provider=provider, sampled=sampled) if used_features is None: used_features = list(range(len(self.original_instance.tokenized))) BaseReturnType.__init__(self, used_features=used_features, labels=labels, labelset=labelset) self._rules = self._extract_rules(rules) def _extract_rules(self, rules: Union[Sequence[str], TreeSurrogate, RuleSurrogate]): if isinstance(rules, (TreeSurrogate, RuleSurrogate)): from skrules.rule import replace_feature_name from skrules.skope_rules import BASE_FEATURE_NAME feature_dict = {BASE_FEATURE_NAME + str(i): feat for i, feat in enumerate(self.used_features)} return [(replace_feature_name(rule, feature_dict), perf) for rule, perf in rules.rules] print(rules) raise NotImplementedError('TODO: Support lists of rules') @property def rules(self): return self._rules","title":"Rules"},{"location":"reference/text_explainability/generation/return_types/#ancestors-in-mro_2","text":"text_explainability.generation.return_types.ReadableDataMixin text_explainability.generation.return_types.BaseReturnType text_explainability.generation.return_types.DataExplanation","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/return_types/#instance-variables_5","text":"1 labels Get labels property. 1 labelset Get label names property. 1 neighborhood_instances Instances in the neighborhood (either sampled or perturbed). 1 original_instance The instance for which the feature attribution scores were calculated. 1 perturbed_instances Perturbed versions of the original instance, if sampled=False during initialization. 1 rules 1 sampled_instances Sampled instances, if sampled=True during initialization. 1 used_features Names of features of the original instance.","title":"Instance variables"},{"location":"reference/text_explainability/generation/return_types/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/return_types/#label_by_index_3","text":"1 2 3 4 def label_by_index ( self , idx : int ) -> Union [ str , int ] Access label name by index, if labelset is set. Parameters: Name Type Description Default idx int Lookup index. None Returns: Type Description Union[str, int] Label name (if available) else index. Raises: Type Description IndexError labelset is set but the element index is not in labelset (index out of bounds). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def label_by_index(self, idx: int) -> Union[str, int]: \"\"\"Access label name by index, if `labelset` is set. Args: idx (int): Lookup index. Raises: IndexError: `labelset` is set but the element index is not in `labelset` (index out of bounds). Returns: Union[str, int]: Label name (if available) else index. \"\"\" if self.labelset is not None: return self.labelset[idx] return idx","title":"label_by_index"},{"location":"reference/text_explainability/generation/surrogate/","text":"Module text_explainability.generation.surrogate Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 \"\"\"Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) \"\"\" import numpy as np from sklearn.base import clone from typing import Optional, Sequence from text_explainability.default import Readable class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): self._model.alpha = 0 def alpha_reset(self): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X) Classes BaseSurrogate 1 2 3 class BaseSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError Ancestors (in MRO) text_explainability.default.Readable Descendants text_explainability.generation.surrogate.LinearSurrogate text_explainability.generation.surrogate.TreeSurrogate text_explainability.generation.surrogate.RuleSurrogate Instance variables 1 feature_importances Methods fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) LinearSurrogate 1 2 3 class LinearSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): self._model.alpha = 0 def alpha_reset(self): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable Instance variables 1 coef 1 feature_importances 1 fit_intercept 1 intercept Methods alpha_reset 1 2 3 def alpha_reset ( self ) View Source 1 2 3 def alpha_reset(self): self._model.alpha = self.__alpha_original alpha_zero 1 2 3 def alpha_zero ( self ) View Source 1 2 3 def alpha_zero(self): self._model.alpha = 0 fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) score 1 2 3 4 5 6 def score ( self , X , y , weights = None ) View Source 1 2 3 def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) RuleSurrogate 1 2 3 class RuleSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X) Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable Instance variables 1 feature_importances 1 feature_names 1 rules Methods fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) score_top_rules 1 2 3 4 def score_top_rules ( self , X ) View Source 1 2 3 def score_top_rules(self, X): return self._model.score_top_rules(X) TreeSurrogate 1 2 3 class TreeSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules Ancestors (in MRO) text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable Instance variables 1 classes 1 feature_importances 1 max_rule_size 1 rules Methods decision_path 1 2 3 4 def decision_path ( self , X ) View Source 1 2 3 4 5 6 7 8 9 10 11 def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() features 1 2 3 4 def features ( self , tokens_to_map : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] fit 1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self leaf_classes 1 2 3 def leaf_classes ( self ) View Source 1 2 3 4 5 6 7 def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] predict 1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X) to_rules 1 2 3 def to_rules ( self ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"Surrogate"},{"location":"reference/text_explainability/generation/surrogate/#module-text_explainabilitygenerationsurrogate","text":"Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 \"\"\"Wrappers for surrogate models, used for local/global explanations. Todo: * Add documentation * Differentiate between classifiers and regressors * Extract rules from decision tree (https://mljar.com/blog/extract-rules-decision-tree/) \"\"\" import numpy as np from sklearn.base import clone from typing import Optional, Sequence from text_explainability.default import Readable class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): self._model.alpha = 0 def alpha_reset(self): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"Module text_explainability.generation.surrogate"},{"location":"reference/text_explainability/generation/surrogate/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/surrogate/#basesurrogate","text":"1 2 3 class BaseSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BaseSurrogate(Readable): def __init__(self, model): super().__init__() self._model = clone(model) def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self def predict(self, X): return self._model.predict(X) @property def feature_importances(self): raise NotImplementedError","title":"BaseSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro","text":"text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#descendants","text":"text_explainability.generation.surrogate.LinearSurrogate text_explainability.generation.surrogate.TreeSurrogate text_explainability.generation.surrogate.RuleSurrogate","title":"Descendants"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables","text":"1 feature_importances","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#fit","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#linearsurrogate","text":"1 2 3 class LinearSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LinearSurrogate(BaseSurrogate): def __init__(self, model): \"\"\"Wrapper around sklearn linear model for usage in local/global surrogate models.\"\"\" super().__init__(model) self.__alpha_original = self._model.alpha @property def coef(self): return self._model.coef_ @property def feature_importances(self): return self.coef @property def intercept(self): return self._model.intercept_ def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights) def alpha_zero(self): self._model.alpha = 0 def alpha_reset(self): self._model.alpha = self.__alpha_original @property def fit_intercept(self): return self._model.fit_intercept @fit_intercept.setter def fit_intercept(self, fit_intercept): self._model.fit_intercept = fit_intercept","title":"LinearSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_1","text":"text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_1","text":"1 coef 1 feature_importances 1 fit_intercept 1 intercept","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#alpha_reset","text":"1 2 3 def alpha_reset ( self ) View Source 1 2 3 def alpha_reset(self): self._model.alpha = self.__alpha_original","title":"alpha_reset"},{"location":"reference/text_explainability/generation/surrogate/#alpha_zero","text":"1 2 3 def alpha_zero ( self ) View Source 1 2 3 def alpha_zero(self): self._model.alpha = 0","title":"alpha_zero"},{"location":"reference/text_explainability/generation/surrogate/#fit_1","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict_1","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#score","text":"1 2 3 4 5 6 def score ( self , X , y , weights = None ) View Source 1 2 3 def score(self, X, y, weights=None): return self._model.score(X, y, sample_weight=weights)","title":"score"},{"location":"reference/text_explainability/generation/surrogate/#rulesurrogate","text":"1 2 3 class RuleSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class RuleSurrogate(BaseSurrogate): \"\"\"Wrapper around `SkopeRules`_ model for usage in local/global surrogate models. _SkopeRules: https://github.com/scikit-learn-contrib/skope-rules \"\"\" @property def rules(self): return self._model.rules_ @property def feature_names(self): return self._model.feature_names @feature_names.setter def feature_names(self, feature_names: Sequence[str]): self._model.feature_names = feature_names def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"RuleSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_2","text":"text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_2","text":"1 feature_importances 1 feature_names 1 rules","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_2","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#fit_2","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#predict_2","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#score_top_rules","text":"1 2 3 4 def score_top_rules ( self , X ) View Source 1 2 3 def score_top_rules(self, X): return self._model.score_top_rules(X)","title":"score_top_rules"},{"location":"reference/text_explainability/generation/surrogate/#treesurrogate","text":"1 2 3 class TreeSurrogate ( model ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class TreeSurrogate(BaseSurrogate): \"\"\"Wrapper around sklearn tree model for usage in local/global surrogate models.\"\"\" @property def feature_importances(self): return self._model.feature_importances_ @property def classes(self): return self._model.classes_ @property def max_rule_size(self): return self._model.max_depth @max_rule_size.setter def max_rule_size(self, size: Optional[int]): self._model.set_params(max_depth=size) @property def rules(self): if not hasattr(self, '_rules') or self._rules is None: self.to_rules() return self._rules def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray() def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature] def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)] def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"TreeSurrogate"},{"location":"reference/text_explainability/generation/surrogate/#ancestors-in-mro_3","text":"text_explainability.generation.surrogate.BaseSurrogate text_explainability.default.Readable","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/surrogate/#instance-variables_3","text":"1 classes 1 feature_importances 1 max_rule_size 1 rules","title":"Instance variables"},{"location":"reference/text_explainability/generation/surrogate/#methods_3","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/surrogate/#decision_path","text":"1 2 3 4 def decision_path ( self , X ) View Source 1 2 3 4 5 6 7 8 9 10 11 def decision_path(self, X): if not isinstance(X, np.ndarray): X = np.array(X) if X.ndim < 2: X = X.reshape(1, -1) return self._model.decision_path(X).toarray()","title":"decision_path"},{"location":"reference/text_explainability/generation/surrogate/#features","text":"1 2 3 4 def features ( self , tokens_to_map : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 def features(self, tokens_to_map: Optional[Sequence[str]] = None): def map_token(token): if tokens_to_map is None: return token return tokens_to_map[token] return [None if f < 0 else map_token[f] for f in self._model.tree_.feature]","title":"features"},{"location":"reference/text_explainability/generation/surrogate/#fit_3","text":"1 2 3 4 5 6 def fit ( self , X , y , weights = None ) View Source 1 2 3 4 5 def fit(self, X, y, weights=None): self._model.fit(X, y, sample_weight=weights) return self","title":"fit"},{"location":"reference/text_explainability/generation/surrogate/#leaf_classes","text":"1 2 3 def leaf_classes ( self ) View Source 1 2 3 4 5 6 7 def leaf_classes(self): # TODO: check if truly classification return [self._model.classes_[np.argmax(self._model.tree_.value[i])] if f < 0 else None for i, f in enumerate(self._model.tree_.feature)]","title":"leaf_classes"},{"location":"reference/text_explainability/generation/surrogate/#predict_3","text":"1 2 3 4 def predict ( self , X ) View Source 1 2 3 def predict(self, X): return self._model.predict(X)","title":"predict"},{"location":"reference/text_explainability/generation/surrogate/#to_rules","text":"1 2 3 def to_rules ( self ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def to_rules(self): from skrules.skope_rules import SkopeRules, BASE_FEATURE_NAME from skrules.rule import Rule feature_names = [BASE_FEATURE_NAME + str(i) for i in range(self._model.n_features_)] rules = SkopeRules._tree_to_rules(None, tree=self._model, feature_names=feature_names) # TODO: add performance metrics and output label self._rules = [tuple(rule) for rule in [Rule(r) for r in rules]] return self._rules","title":"to_rules"},{"location":"reference/text_explainability/generation/target_encoding/","text":"Module text_explainability.generation.target_encoding Encode targets into binary labels for contrastive explanation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \"\"\"Encode targets into binary labels for contrastive explanation.\"\"\" import numpy as np from typing import Optional, Sequence, Union, Generator, List from instancelib.machinelearning import AbstractClassifier class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] Classes FactFoilEncoder 1 2 3 4 class FactFoilEncoder ( foil : int , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] Ancestors (in MRO) text_explainability.generation.target_encoding.TargetEncoder Static methods from_str 1 2 3 4 def from_str ( label : str , labelset : Union [ instancelib . machinelearning . base . AbstractClassifier , Sequence [ str ]] ) Instantiate FactFoilEncoder with a string as foil. Parameters: Name Type Description Default label str Foil (expected outcome) label. None labelset Union[AbstractClassifier, Sequence[str]] Labelset containing the foil. None Returns: Type Description FactFoilEncoder Initialized FactFoilEncoder. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) Instance variables 1 labelset Methods encode 1 2 3 4 def encode ( self , y ) Encode a single instance into foil (0) or not foil (1). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y] get_label 1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y TargetEncoder 1 2 3 class TargetEncoder ( labels : Union [ Sequence [ str ], instancelib . machinelearning . base . AbstractClassifier , NoneType ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) Descendants text_explainability.generation.target_encoding.FactFoilEncoder Instance variables 1 labelset Methods encode 1 2 3 4 def encode ( self , y ) Encode a single instance. View Source 1 2 3 4 5 def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y get_label 1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"Target Encoding"},{"location":"reference/text_explainability/generation/target_encoding/#module-text_explainabilitygenerationtarget_encoding","text":"Encode targets into binary labels for contrastive explanation. None View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 \"\"\"Encode targets into binary labels for contrastive explanation.\"\"\" import numpy as np from typing import Optional, Sequence, Union, Generator, List from instancelib.machinelearning import AbstractClassifier class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True)) class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"Module text_explainability.generation.target_encoding"},{"location":"reference/text_explainability/generation/target_encoding/#classes","text":"","title":"Classes"},{"location":"reference/text_explainability/generation/target_encoding/#factfoilencoder","text":"1 2 3 4 class FactFoilEncoder ( foil : int , labelset : Optional [ Sequence [ str ]] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class FactFoilEncoder(TargetEncoder): def __init__(self, foil: int, labelset: Optional[Sequence[str]] = None): super().__init__(labelset) self.foil = foil @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset) def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"FactFoilEncoder"},{"location":"reference/text_explainability/generation/target_encoding/#ancestors-in-mro","text":"text_explainability.generation.target_encoding.TargetEncoder","title":"Ancestors (in MRO)"},{"location":"reference/text_explainability/generation/target_encoding/#static-methods","text":"","title":"Static methods"},{"location":"reference/text_explainability/generation/target_encoding/#from_str","text":"1 2 3 4 def from_str ( label : str , labelset : Union [ instancelib . machinelearning . base . AbstractClassifier , Sequence [ str ]] ) Instantiate FactFoilEncoder with a string as foil. Parameters: Name Type Description Default label str Foil (expected outcome) label. None labelset Union[AbstractClassifier, Sequence[str]] Labelset containing the foil. None Returns: Type Description FactFoilEncoder Initialized FactFoilEncoder. View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @classmethod def from_str(cls, label: str, labelset: Union[AbstractClassifier, Sequence[str]]): \"\"\"Instantiate FactFoilEncoder with a string as foil. Args: label (str): Foil (expected outcome) label. labelset (Union[AbstractClassifier, Sequence[str]]): Labelset containing the foil. Returns: FactFoilEncoder: Initialized FactFoilEncoder. \"\"\" if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) foil = labelset.index(label) return cls(foil, labelset)","title":"from_str"},{"location":"reference/text_explainability/generation/target_encoding/#instance-variables","text":"1 labelset","title":"Instance variables"},{"location":"reference/text_explainability/generation/target_encoding/#methods","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/target_encoding/#encode","text":"1 2 3 4 def encode ( self , y ) Encode a single instance into foil (0) or not foil (1). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 def encode(self, y): \"\"\"Encode a single instance into foil (0) or not foil (1).\"\"\" if all(isinstance(y_, str) for y_ in y): y = [self.labelset.index(y_) for y_ in y] if isinstance(self.foil, int): return [0 if y_ == self.foil else 1 for y_ in y] return [0 if y_ in self.foil else 1 for y_ in y]","title":"encode"},{"location":"reference/text_explainability/generation/target_encoding/#get_label","text":"1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"get_label"},{"location":"reference/text_explainability/generation/target_encoding/#targetencoder","text":"1 2 3 class TargetEncoder ( labels : Union [ Sequence [ str ], instancelib . machinelearning . base . AbstractClassifier , NoneType ] = None ) View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class TargetEncoder: def __init__(self, labels: Optional[Union[Sequence[str], AbstractClassifier]] = None): \"\"\"Encode model predictions based on encoding rule. Args: labels (Optional[Union[Sequence[str], AbstractClassifier]], optional): Labelset for mapping labels onto. Defaults to None. \"\"\" self.labelset = labels @property def labelset(self): return self.__labelset @labelset.setter def labelset(self, labelset): if isinstance(labelset, AbstractClassifier): labelset = labelset.encoder.labelset if isinstance(labelset, frozenset): labelset = list(labelset) self.__labelset = labelset def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y def __call__(self, y) -> List[int]: \"\"\"Encode multiple predicted labels. Args: y: Predictions with optional indices. Returns: List[int]: Encoded labels as indices. \"\"\" return self.encode(self.get_label(y, proba_to_labels=True, label_to_index=True))","title":"TargetEncoder"},{"location":"reference/text_explainability/generation/target_encoding/#descendants","text":"text_explainability.generation.target_encoding.FactFoilEncoder","title":"Descendants"},{"location":"reference/text_explainability/generation/target_encoding/#instance-variables_1","text":"1 labelset","title":"Instance variables"},{"location":"reference/text_explainability/generation/target_encoding/#methods_1","text":"","title":"Methods"},{"location":"reference/text_explainability/generation/target_encoding/#encode_1","text":"1 2 3 4 def encode ( self , y ) Encode a single instance. View Source 1 2 3 4 5 def encode(self, y): \"\"\"Encode a single instance.\"\"\" return y","title":"encode"},{"location":"reference/text_explainability/generation/target_encoding/#get_label_1","text":"1 2 3 4 5 6 def get_label ( self , y , proba_to_labels : bool = True , label_to_index : bool = True ) -> Union [ List [ int ], List [ str ]] Get prediction label as probability, string or class index. Parameters: Name Type Description Default y None Predictions with optional indices. None proba_to_labels bool Whether to convert probability to highest scoring class. Defaults to True. True label_to_index bool Convert string to index in labelset. Defaults to True. True Returns: Type Description Union[List[int], List[str]] Label names (if label_to_index is False) or label indices (otherwise). View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def get_label(self, y, proba_to_labels: bool = True, label_to_index: bool = True) -> Union[List[int], List[str]]: \"\"\"Get prediction label as probability, string or class index. Args: y: Predictions with optional indices. proba_to_labels (bool, optional): Whether to convert probability to highest scoring class. Defaults to True. label_to_index (bool, optional): Convert string to index in labelset. Defaults to True. Returns: Union[List[int], List[str]]: Label names (if label_to_index is False) or label indices (otherwise). \"\"\" if isinstance(y, Generator): y = list(y) if len(y) > 0: if isinstance(y[0], tuple): y = [y_[1] for y_ in y] if isinstance(y[0], frozenset): y = [list(y_) for y_ in y] if isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], str): y = [y_[0] for y_ in y] if proba_to_labels: # model.predict_proba if len(y) > 0 and isinstance(y[0], list) and len(y[0]) > 0 and isinstance(y[0][0], tuple): y = [sorted(y_, key=lambda x: x[1], reverse=True)[0][0] for y_ in y] # model.predict_proba_raw if len(y) > 0 and isinstance(y[0], np.ndarray): y = np.hstack([np.argmax(y_, axis=1) for y_ in y]) if label_to_index: return [self.labelset.index(y_) if isinstance(y_, str) else y_ for y_ in y] return y","title":"get_label"}]}